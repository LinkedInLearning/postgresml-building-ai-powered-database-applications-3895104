# -*- coding: utf-8 -*-
"""PGML-Capstone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DzzgwUxJQLA1Nu1V_ThQTKETM749-da4
"""

# CELL 0: CAPSTONE PROJECT - END-TO-END PRODUCT RECOMMENDER
#@markdown # Capstone: Building & "Deploying" an End-to-End Product Recommender with PostgresML
#@markdown ## Project Goal:
#@markdown This project walks through the creation of a product recommendation system using PostgresML. We will cover:
#@markdown 1. Generating synthetic data for users, products, and interactions.
#@markdown 2. Engineering features for users and products.
#@markdown 3. Preparing training data for a recommendation model.
#@markdown 4. Training a regression-based recommendation model using `pgml.train`.
#@markdown 5. Creating a SQL function to get recommendations using `pgml.predict`.
#@markdown 6. Designing a FastAPI endpoint to serve these recommendations.
#@markdown 7. Implementing basic SQL-based monitoring and security constructs.
#@markdown
#@markdown ## Learning Objectives Alignment:
#@markdown This capstone aims to apply concepts of data preparation, feature engineering, model training (`pgml.train`), model serving (`pgml.predict` via SQL functions), API integration (FastAPI design), and basic production considerations (monitoring, security) as covered in the course.
#@markdown ---
#@markdown ### Instructions:
#@markdown 1. **Set Connection String:** Ensure `DB_CONNECTION_STRING` in Cell 1 is correct.
#@markdown 2. **Run Cells Sequentially:** Execute each cell in order.
#@markdown 3. **Observe Outputs:** Pay attention to print statements and DataFrame outputs.
#@markdown ---

import IPython
display(IPython.display.Markdown("## Let's Build an End-to-End Product Recommendation System!"))

# CELL 1: CONFIGURATION - DATABASE CONNECTION STRING
#@markdown ### PostgreSQL Connection String
#@markdown Enter your full PostgresML connection string below.
DB_CONNECTION_STRING = "postgres://u_zvtt5tif9x1yz0l:xjeuxoo9levuxmm@437a9d42-c398-4c00-9906-c9b5fc2e7d61.gcp.db.postgresml.org:6432/pgml_dhrhajumjl7tmkm" #@param {type:"string"}

# --- Sanity check for the connection string ---
if 'DB_CONNECTION_STRING' not in globals() or not DB_CONNECTION_STRING or "postgres://" not in DB_CONNECTION_STRING:
    print("üî¥ ERROR: DB_CONNECTION_STRING is not set or does not look like a valid PostgreSQL connection string.")
    print("Please set it correctly in this cell before proceeding.")
    # raise ValueError("DB_CONNECTION_STRING not configured properly.") # Optional: Stop execution
else:
    try:
        user_part = DB_CONNECTION_STRING.split("://")[1].split(":")[0]
        host_part = DB_CONNECTION_STRING.split("@")[1]
        masked_conn_string = f"postgres://{user_part}:********@{host_part}"
    except Exception:
        masked_conn_string = "postgres://USER:********@HOST/DATABASE (Could not fully parse to mask)"
    print(f"‚úÖ DB_CONNECTION_STRING is set (password masked for display): {masked_conn_string}")
    print("You can now run the next cell to install dependencies and define helpers.")

# CELL 2: INSTALL DEPENDENCIES & DEFINE HELPER FUNCTIONS
!pip install psycopg2-binary Faker pandas -q
!apt-get update -qq && apt-get install -y postgresql-client -qq

import psycopg2
import json
import pandas as pd
from faker import Faker
import textwrap
import time
import random
from datetime import datetime, timedelta
import sys # For flushing output

# --- Enhanced Helper function to run SQL commands ---
def run_sql(sql_query, params=None, fetch_results=True, conn_string=None, quiet=False):
    if not quiet:
        print(f" Kicking off run_sql")
        sys.stdout.flush()
    if conn_string is None: conn_string = DB_CONNECTION_STRING
    if not conn_string:
        if not quiet: print("üî¥ run_sql: DB_CONNECTION_STRING is not available.")
        return pd.DataFrame()
    conn = None
    try:
        if not quiet: print(f"  Connecting to DB...")
        sys.stdout.flush()
        conn = psycopg2.connect(conn_string)
        # No pgvector needed for this specific capstone version
        if not quiet: print(f"  Connection successful. Executing query...")
        sys.stdout.flush()
        with conn.cursor() as cur:
            cur.execute(sql_query, params)
            if not quiet: print(f"  Query executed.")
            sys.stdout.flush()
            if fetch_results and cur.description:
                columns = [desc[0] for desc in cur.description]
                results_data = cur.fetchall() # <<<< ERROR MIGHT HAPPEN HERE if function errors
                if not quiet: print(f"  Fetched {len(results_data)} rows.")
                sys.stdout.flush()
                return pd.DataFrame(results_data, columns=columns) if results_data else pd.DataFrame(columns=columns)
            conn.commit() # Commit for DDL/DML if fetch_results is false or no description
            if not quiet: print(f"  Committed (if applicable).")
            sys.stdout.flush()
        return pd.DataFrame()
    except psycopg2.Error as e:
        if not quiet:
            print(f"üî¥ SQL Error in run_sql:")
            print(f"   Query: {sql_query[:500]}{'...' if len(sql_query) > 500 else ''}")
            if params: print(f"   Params: {params}")
            print(f"   Error Code: {e.pgcode}") # Standard error code
            print(f"   Raw pgerror: {e.pgerror}") # Raw error message from DB
            # Try to get more detailed diagnostic message if available
            if hasattr(e, 'diag') and e.diag and hasattr(e.diag, 'message_primary'):
                print(f"   Primary Message: {e.diag.message_primary}")
            if hasattr(e, 'diag') and e.diag and hasattr(e.diag, 'message_detail'):
                print(f"   Detail Message: {e.diag.message_detail}")
            if hasattr(e, 'diag') and e.diag and hasattr(e.diag, 'message_hint'):
                print(f"   Hint: {e.diag.message_hint}")
            print(f"   Full Python Error: {e}")
            sys.stdout.flush()
        if conn: conn.rollback()
    except Exception as ex:
        if not quiet: print(f"üî¥ Non-SQL Error in run_sql: {ex}"); sys.stdout.flush()
    finally:
        if conn:
            conn.close()
        # Corrected indentation for this block
        if not quiet: print(f"  Database connection closed."); sys.stdout.flush()
    return pd.DataFrame()

def print_wrapped(text, width=100): print(textwrap.fill(str(text), width=width))
print("\n‚úÖ Dependencies installed and helper functions defined.")

# CELL 3: VERIFY POSTGRESML EXTENSION & GET VERSION
print("--- Verifying pgml Extension and Getting Version ---")

# Check for pgml
print("\nChecking for 'pgml' extension...")
pgml_check_sql = "SELECT extname FROM pg_extension WHERE extname = 'pgml';"
pgml_df = run_sql(pgml_check_sql, quiet=True)
if not pgml_df.empty and 'pgml' in pgml_df['extname'].values:
    print("‚úÖ 'pgml' extension is enabled.")
else:
    print("‚ö†Ô∏è 'pgml' extension not found. Attempting to enable...")
    run_sql("CREATE EXTENSION IF NOT EXISTS pgml;", fetch_results=False, quiet=True)
    time.sleep(1)
    pgml_df = run_sql(pgml_check_sql, quiet=True)
    if not pgml_df.empty and 'pgml' in pgml_df['extname'].values:
         print("‚úÖ 'pgml' extension successfully enabled.")
    else:
        print("üî¥ ERROR: Failed to enable or find 'pgml' extension. This is required.")

# Get PostgresML version
print("\n--- Attempting to get PostgresML version ---")
COURSE_PGML_VERSION = "Unknown"
try:
    version_df = run_sql("SELECT pgml.version();", quiet=True)
    if not version_df.empty:
        COURSE_PGML_VERSION = version_df.iloc[0][0]
        print(f"‚úÖ PostgresML Version: {COURSE_PGML_VERSION}")
    else:
        print("‚ö†Ô∏è pgml.version() did not return results.")
except Exception as e:
    print(f"‚ö†Ô∏è Could not get pgml.version(): {e}")

print("\n--- End of Cell 3 ---")

# CELL 4: DATA GENERATION & SCHEMA SETUP
#@markdown Generates synthetic data for users, products, and interactions, then creates the necessary tables.
print("--- Generating Synthetic Data & Setting Up Schema ---")

from faker import Faker
fake = Faker()
Faker.seed(0) # for reproducibility
random.seed(0)

#@markdown #### Data Generation Parameters:
NUM_USERS = 150 #@param {type:"integer"}
NUM_PRODUCTS = 70 #@param {type:"integer"}
NUM_INTERACTIONS = 750 #@param {type:"integer"}

print(f"Parameters: {NUM_USERS} users, {NUM_PRODUCTS} products, {NUM_INTERACTIONS} interactions.")

# --- Cleanup existing tables (order matters for FKs, or use CASCADE) ---
print("\nDropping existing tables (if any)...")
tables_to_drop = [
    "monitoring.predictions", "monitoring.metrics_view",
    "training_data", "interactions",
    "user_features", "product_features",
    "users", "products"
]
for table_name in tables_to_drop:
    if table_name.endswith("_view"):
        run_sql(f"DROP VIEW IF EXISTS {table_name} CASCADE;", fetch_results=False, quiet=True)
    else:
        run_sql(f"DROP TABLE IF EXISTS {table_name} CASCADE;", fetch_results=False, quiet=True)
run_sql("DROP SCHEMA IF EXISTS monitoring CASCADE;", fetch_results=False, quiet=True)
print("‚úÖ Dropped existing project-related tables and monitoring schema.")

# --- Create Tables ---
print("\nCreating database tables...")
run_sql("CREATE TABLE users (user_id INTEGER PRIMARY KEY, age INTEGER, gender TEXT, country_code TEXT);", fetch_results=False)
# ADDED product_name to products table
run_sql("CREATE TABLE products (product_id INTEGER PRIMARY KEY, product_name TEXT, category TEXT, brand TEXT, price FLOAT);", fetch_results=False)
run_sql("""
    CREATE TABLE interactions (
        interaction_id SERIAL PRIMARY KEY,
        user_id INTEGER REFERENCES users(user_id),
        product_id INTEGER REFERENCES products(product_id),
        interaction_type TEXT,
        rating FLOAT,
        timestamp TIMESTAMP DEFAULT NOW()
    );""", fetch_results=False)
print("‚úÖ Core tables created: users, products, interactions.")

# --- Populate Users ---
print("\nPopulating 'users' table...")
user_data = [(i, random.randint(18, 70), random.choice(['Male', 'Female', 'Other']), fake.country_code()) for i in range(1, NUM_USERS + 1)]
for u_data in user_data: run_sql("INSERT INTO users VALUES (%s, %s, %s, %s);", u_data, fetch_results=False, quiet=True)
print(f"‚úÖ {len(user_data)} users inserted.")

# --- Populate Products ---
print("\nPopulating 'products' table...")
categories = ['Electronics', 'Books', 'Clothing', 'Home & Kitchen', 'Sports & Outdoors', 'Beauty', 'Toys']
brands = [fake.company().split(' ')[0].replace(',', '') for _ in range(15)]
product_data = []
for i in range(1, NUM_PRODUCTS + 1):
    cat = random.choice(categories)
    # Generate a plausible product name
    prod_name = f"{random.choice(['Premium', 'Basic', 'Advanced', 'Eco', 'Pro'])} {cat.replace(' & ', '_').split(' ')[0]} {fake.word().capitalize()} {random.choice(['Widget', 'Device', 'Gear', 'Essentials', 'Kit'])}"
    product_data.append((i, prod_name, cat, random.choice(brands), round(random.uniform(5.0, 1000.0), 2)))

for p_data in product_data: run_sql("INSERT INTO products (product_id, product_name, category, brand, price) VALUES (%s, %s, %s, %s, %s);", p_data, fetch_results=False, quiet=True)
print(f"‚úÖ {len(product_data)} products inserted.")

# --- Populate Interactions ---
# (Interaction population logic remains the same)
print("\nPopulating 'interactions' table...")
interaction_types = ['view'] * 7 + ['add_to_cart'] * 2 + ['purchase'] * 1 # Skew towards views
interaction_data = []
for _ in range(NUM_INTERACTIONS):
    user_id = random.randint(1, NUM_USERS)
    product_id = random.randint(1, NUM_PRODUCTS)
    itype = random.choice(interaction_types)
    rating = None
    if itype == 'purchase': rating = round(random.uniform(3.5, 5.0), 1)
    elif itype == 'add_to_cart': rating = round(random.uniform(2.5, 4.5), 1)
    elif random.random() < 0.1: rating = round(random.uniform(1.0, 5.0), 1)
    ts = datetime.now() - timedelta(days=random.randint(0, 180), hours=random.randint(0,23), minutes=random.randint(0,59))
    interaction_data.append((user_id, product_id, itype, rating, ts))

for i_data in interaction_data: run_sql("INSERT INTO interactions (user_id, product_id, interaction_type, rating, timestamp) VALUES (%s, %s, %s, %s, %s);", i_data, fetch_results=False, quiet=True)
print(f"‚úÖ {len(interaction_data)} interactions inserted.")


# --- Sample Data Verification ---
print("\n--- Sample Data Verification ---")
print("Sample Users (first 3):"); print(run_sql("SELECT * FROM users LIMIT 3;").to_string())
print("\nSample Products (first 3):"); print(run_sql("SELECT product_id, product_name, category FROM products LIMIT 3;").to_string()) # Added product_name
print("\nSample Interactions (first 3, with product name for context):")
sample_interactions_sql = """
SELECT i.user_id, i.product_id, p.product_name, i.interaction_type, i.rating
FROM interactions i JOIN products p ON i.product_id = p.product_id
ORDER BY i.timestamp DESC LIMIT 3;
"""
print(run_sql(sample_interactions_sql).to_string())
print("\n--- End of Cell 4 ---")

# CELL 5: FEATURE ENGINEERING & TRAINING DATA PREPARATION (WITH SCALING & PRINT FIX)
#@markdown Creates feature tables, scales numerical features (age, price), and the final `training_data` table.
print("--- Feature Engineering & Training Data Preparation (with Scaling) ---")

# --- Get Min/Max for scaling from original tables ---
print("\nFetching min/max for Age and Price for scaling...")
age_stats_df = run_sql("SELECT MIN(age) as min_age, MAX(age) as max_age FROM users;", quiet=True)
price_stats_df = run_sql("SELECT MIN(price) as min_price, MAX(price) as max_price FROM products;", quiet=True)

MIN_AGE = age_stats_df.iloc[0]['min_age'] if not age_stats_df.empty and age_stats_df.iloc[0]['min_age'] is not None else 18.0
MAX_AGE = age_stats_df.iloc[0]['max_age'] if not age_stats_df.empty and age_stats_df.iloc[0]['max_age'] is not None else 70.0
MIN_PRICE = price_stats_df.iloc[0]['min_price'] if not price_stats_df.empty and price_stats_df.iloc[0]['min_price'] is not None else 0.0
MAX_PRICE = price_stats_df.iloc[0]['max_price'] if not price_stats_df.empty and price_stats_df.iloc[0]['max_price'] is not None else 1000.0

if MAX_AGE == MIN_AGE: MAX_AGE = MIN_AGE + 1.0
if MAX_PRICE == MIN_PRICE: MAX_PRICE = MIN_PRICE + 1.0
DEFAULT_AGE_SCALED = ( (MIN_AGE + (MAX_AGE-MIN_AGE)/2) - MIN_AGE) / (MAX_AGE - MIN_AGE) if (MAX_AGE - MIN_AGE) != 0 else 0.5
DEFAULT_PRICE_SCALED = ( (MIN_PRICE + (MAX_PRICE-MIN_PRICE)/2) - MIN_PRICE) / (MAX_PRICE - MIN_PRICE) if (MAX_PRICE - MIN_PRICE) != 0 else 0.5

print(f"Scaling params: Age (min:{MIN_AGE}, max:{MAX_AGE}), Price (min:{MIN_PRICE}, max:{MAX_PRICE})")

print("\nDropping existing feature and training data tables...")
run_sql("DROP TABLE IF EXISTS training_data CASCADE;", fetch_results=False, quiet=True)
run_sql("DROP TABLE IF EXISTS user_features CASCADE;", fetch_results=False, quiet=True)
run_sql("DROP TABLE IF EXISTS product_features CASCADE;", fetch_results=False, quiet=True)
print("‚úÖ Dropped tables.")

print("\nCreating and populating 'user_features' table (with Age scaling)...")
run_sql("""CREATE TABLE user_features (user_id INTEGER PRIMARY KEY REFERENCES users(user_id) ON DELETE CASCADE, features FLOAT[]);""", fetch_results=False)
populate_user_features_sql = f"""
INSERT INTO user_features (user_id, features)
SELECT user_id, ARRAY[
        CASE WHEN {MAX_AGE} - {MIN_AGE} = 0 THEN {DEFAULT_AGE_SCALED} ELSE (COALESCE(age::FLOAT, {MIN_AGE + (MAX_AGE-MIN_AGE)/2}) - {MIN_AGE}) / ({MAX_AGE} - {MIN_AGE}) END,
        (CASE WHEN gender = 'Male' THEN 1.0 ELSE 0.0 END), (CASE WHEN gender = 'Female' THEN 1.0 ELSE 0.0 END), (CASE WHEN gender = 'Other' THEN 1.0 ELSE 0.0 END)
    ] FROM users;"""
run_sql(populate_user_features_sql, fetch_results=False, quiet=True)
print(f"‚úÖ 'user_features' populated. Sample:")
user_features_sample_df = run_sql("SELECT user_id, features FROM user_features LIMIT 3;")
print(user_features_sample_df.to_string())
USER_FEATURE_LENGTH = user_features_sample_df.iloc[0]['features'].__len__() if not user_features_sample_df.empty and user_features_sample_df.iloc[0]['features'] is not None else 0
if USER_FEATURE_LENGTH > 0: print(f"==> Detected User Feature Length: {USER_FEATURE_LENGTH}")

print("\nCreating and populating 'product_features' table (with Price scaling)...")
run_sql("""CREATE TABLE product_features (product_id INTEGER PRIMARY KEY REFERENCES products(product_id) ON DELETE CASCADE, features FLOAT[]);""", fetch_results=False)
categories_df = run_sql("SELECT DISTINCT category FROM products ORDER BY category;", quiet=True)
categories_list = categories_df['category'].tolist() if not categories_df.empty else []
print(f"Product categories for one-hot encoding: {categories_list}")
product_features_parts_sql_list = [f"CASE WHEN {MAX_PRICE} - {MIN_PRICE} = 0 THEN {DEFAULT_PRICE_SCALED} ELSE (COALESCE(price::FLOAT, {MIN_PRICE + (MAX_PRICE-MIN_PRICE)/2}) - {MIN_PRICE}) / ({MAX_PRICE} - {MIN_PRICE}) END"]
for cat_name in categories_list:
    escaped_cat_name = cat_name.replace("'", "''")
    product_features_parts_sql_list.append(f"(CASE WHEN category = '{escaped_cat_name}' THEN 1.0 ELSE 0.0 END)")
product_features_array_sql_expr = "ARRAY[" + ", ".join(product_features_parts_sql_list) + "]"
populate_product_features_sql = f"INSERT INTO product_features (product_id, features) SELECT product_id, {product_features_array_sql_expr} FROM products;"
run_sql(populate_product_features_sql, fetch_results=False, quiet=True)
print(f"‚úÖ 'product_features' populated. Sample:")
product_features_sample_df = run_sql("SELECT product_id, features FROM product_features LIMIT 3;")
print(product_features_sample_df.to_string())
PRODUCT_FEATURE_LENGTH = product_features_sample_df.iloc[0]['features'].__len__() if not product_features_sample_df.empty and product_features_sample_df.iloc[0]['features'] is not None else 0
if PRODUCT_FEATURE_LENGTH > 0: print(f"==> Detected Product Feature Length: {PRODUCT_FEATURE_LENGTH}")

print("\nCreating and populating 'training_data' table...")
run_sql("""CREATE TABLE training_data (interaction_id INTEGER PRIMARY KEY REFERENCES interactions(interaction_id) ON DELETE CASCADE, features FLOAT[], target FLOAT);""", fetch_results=False)
populate_training_data_sql = """INSERT INTO training_data (interaction_id, features, target)
SELECT i.interaction_id, uf.features || pf.features, i.rating FROM interactions i
JOIN user_features uf ON i.user_id = uf.user_id JOIN product_features pf ON i.product_id = pf.product_id
WHERE i.rating IS NOT NULL AND uf.features IS NOT NULL AND pf.features IS NOT NULL;"""
run_sql(populate_training_data_sql, fetch_results=False, quiet=True)
training_data_count_df = run_sql("SELECT COUNT(*) as count, AVG(array_length(features, 1)) as avg_num_features FROM training_data;", quiet=True)
TRAINING_FEATURE_LENGTH = None
avg_feat_len_val = None # Renamed to avoid conflict
if not training_data_count_df.empty:
    print(f"‚úÖ 'training_data' populated with {training_data_count_df.iloc[0]['count']} samples.")
    avg_feat_len_val = training_data_count_df.iloc[0]['avg_num_features']
    if avg_feat_len_val is not None:
        TRAINING_FEATURE_LENGTH = int(avg_feat_len_val)
        print(f"   Average (and expected) number of features per sample: {TRAINING_FEATURE_LENGTH}")
    else: print("   Could not determine average number of features.")
else: print("üî¥ Failed to populate training_data or count it.")

print("Sample training data (first_feat, last_feat if possible, target):") # Corrected print statement logic below
if TRAINING_FEATURE_LENGTH is not None and TRAINING_FEATURE_LENGTH > 0:
    last_feature_index = TRAINING_FEATURE_LENGTH
    sample_training_data_sql = f"SELECT interaction_id, features[1] as first_feat, features[{last_feature_index}] as last_feat, target FROM training_data WHERE features IS NOT NULL AND array_length(features,1) = {TRAINING_FEATURE_LENGTH} LIMIT 3;"
    print(run_sql(sample_training_data_sql).to_string())
else:
    sample_training_data_sql_fallback = "SELECT interaction_id, features[1] as first_feat, target FROM training_data WHERE features IS NOT NULL AND array_length(features,1) >= 1 LIMIT 3;"
    print(run_sql(sample_training_data_sql_fallback).to_string())
    print(" (Could not determine last feature index as TRAINING_FEATURE_LENGTH was unknown or zero)")

if USER_FEATURE_LENGTH > 0 and PRODUCT_FEATURE_LENGTH > 0 and TRAINING_FEATURE_LENGTH is not None:
    if USER_FEATURE_LENGTH + PRODUCT_FEATURE_LENGTH == TRAINING_FEATURE_LENGTH:
        print(f"‚úÖ Sanity Check: User features ({USER_FEATURE_LENGTH}) + Product features ({PRODUCT_FEATURE_LENGTH}) = Training features ({TRAINING_FEATURE_LENGTH}). Lengths match!")
    else:
        print(f"üî¥ WARNING Sanity Check: User features ({USER_FEATURE_LENGTH}) + Product features ({PRODUCT_FEATURE_LENGTH}) = {USER_FEATURE_LENGTH + PRODUCT_FEATURE_LENGTH} != Training features ({TRAINING_FEATURE_LENGTH}). Length mismatch!")

print("\n--- Detailed Inspection of 'training_data' for problematic values ---")
nan_inf_check_sql = "SELECT SUM(CASE WHEN EXISTS (SELECT 1 FROM unnest(features) f WHERE f = 'Infinity'::float OR f = '-Infinity'::float OR f = 'NaN'::float) THEN 1 ELSE 0 END) as rows_with_nan_or_inf_in_features FROM training_data;"
nan_inf_df = run_sql(nan_inf_check_sql)
if not nan_inf_df.empty and nan_inf_df.iloc[0]['rows_with_nan_or_inf_in_features'] is not None:
    print("NaN/Infinity Check in 'features' array elements:"); print(nan_inf_df.to_string())
    if nan_inf_df.iloc[0]['rows_with_nan_or_inf_in_features'] > 0: print("üî¥ CRITICAL: Found rows with NaN or Infinity in features array!")
else: print("Could not perform NaN/Infinity check on features.")
nan_inf_target_check_sql = "SELECT SUM(CASE WHEN target = 'Infinity'::float OR target = '-Infinity'::float OR target = 'NaN'::float THEN 1 ELSE 0 END) as nan_or_inf_in_target FROM training_data;"
nan_inf_target_df = run_sql(nan_inf_target_check_sql)
if not nan_inf_target_df.empty and nan_inf_target_df.iloc[0]['nan_or_inf_in_target'] is not None:
    print("\nNaN/Infinity Check in 'target' column:"); print(nan_inf_target_df.to_string())
    if nan_inf_target_df.iloc[0]['nan_or_inf_in_target'] > 0: print("üî¥ CRITICAL: Found NaN or Infinity in target column!")
else: print("Could not perform NaN/Infinity check on target.")

print("\n--- Verifying scaled feature ranges (Age and Price elements should be ~0-1) ---")
scaled_features_sample_df = run_sql("SELECT features FROM training_data LIMIT 5;")
if not scaled_features_sample_df.empty:
    print("Sample of first few feature arrays in training_data (after scaling):")
    for idx, row_val in scaled_features_sample_df.iterrows(): # Changed row to row_val
        print(f"  Row {idx}: {row_val['features']}")
else: print("Could not fetch scaled features for verification.")
print("\n--- End of Cell 5 ---")

# CELL 6: MODEL TRAINING WITH `pgml.train`
#@markdown Trains a regression model to predict product ratings using the prepared `training_data`.

PROJECT_NAME = 'product_recommender_v1' #@param {type:"string"}
CHOSEN_ALGORITHM = 'linear' #@param ["linear", "xgboost", "lightgbm"]
EXPLICIT_AUTO_DEPLOY = True #@param {type:"boolean"}
ADD_MINIMAL_HYPERPARAMS = False #@param {type:"boolean"}

print(f"--- Training Recommendation Model (Project: '{PROJECT_NAME}', Algorithm: '{CHOSEN_ALGORITHM}', AutoDeploy: {EXPLICIT_AUTO_DEPLOY}, MinimalHyperparams: {ADD_MINIMAL_HYPERPARAMS}) ---")

print("\n--- Verifying 'training_data' content before training (condensed) ---")
training_data_check_sql = """
SELECT COUNT(*) as training_samples
FROM training_data
WHERE features IS NOT NULL AND target IS NOT NULL
  AND NOT EXISTS (SELECT 1 FROM unnest(features) f WHERE f IS NULL OR f = 'NaN'::float OR f = 'Infinity'::float OR f = '-Infinity'::float)
  AND target NOT IN ('NaN'::float, 'Infinity'::float, '-Infinity'::float);"""
training_data_check_df = run_sql(training_data_check_sql, quiet=True)
valid_training_samples = 0
if not training_data_check_df.empty and training_data_check_df.iloc[0]['training_samples'] is not None:
    valid_training_samples = training_data_check_df.iloc[0]['training_samples']

if valid_training_samples == 0:
    print("üî¥ CRITICAL: No valid training samples found. Cannot train model.")
    raise ValueError("No valid training data available.")
else:
    print(f"Found {valid_training_samples} presumably clean training samples.")
    if 'TRAINING_FEATURE_LENGTH' in globals() and TRAINING_FEATURE_LENGTH is not None:
        print(f"   Expected feature length from Cell 5: {TRAINING_FEATURE_LENGTH}")

    print(f"\nStarting model training for project '{PROJECT_NAME}' using 'regression' task and algorithm '{CHOSEN_ALGORITHM}'...")

    hyperparams_sql_string = ""
    if ADD_MINIMAL_HYPERPARAMS:
        if CHOSEN_ALGORITHM == 'xgboost':
            hyperparams_sql_string = ", hyperparams => '{{\"n_estimators\": 20, \"max_depth\": 4, \"learning_rate\": 0.1}}'::jsonb"
            print("   Using minimal hyperparameters for xgboost.")
        elif CHOSEN_ALGORITHM == 'lightgbm':
             hyperparams_sql_string = ", hyperparams => '{{\"n_estimators\": 20, \"num_leaves\": 10, \"learning_rate\": 0.1}}'::jsonb"
             print("   Using minimal hyperparameters for lightgbm.")
        elif CHOSEN_ALGORITHM == 'linear':
            hyperparams_sql_string = ", hyperparams => '{{}}'::jsonb"
            print("   Using empty hyperparameters for linear.")

    train_statement_sql = f"""
    SELECT pgml.train(
        project_name => '{PROJECT_NAME}',
        task => 'regression',
        relation_name => 'training_data',
        y_column_name => 'target',
        algorithm => '{CHOSEN_ALGORITHM}',
        automatic_deploy => {str(EXPLICIT_AUTO_DEPLOY).lower()}
        {hyperparams_sql_string}
    );"""
    print(f"\nExecuting training SQL:\n{train_statement_sql}")
    training_start_time = time.time()
    run_sql(train_statement_sql, fetch_results=False, quiet=False)
    training_duration = time.time() - training_start_time
    print(f"\n--- pgml.train Statement Executed (Duration: {training_duration:.2f} seconds) ---")
    print("If no SQL errors were printed by run_sql, pgml.train command was sent. Now checking pgml metadata & last_error_message...")

    print("\nChecking for any specific pgml error messages (if pgml.last_error_message function exists)...")
    last_error_df = run_sql("SELECT pgml.last_error_message();", quiet=True)
    if not last_error_df.empty and last_error_df.iloc[0,0] is not None and str(last_error_df.iloc[0,0]).strip() != '':
        print(f"‚ö†Ô∏è pgml.last_error_message() reported: {last_error_df.iloc[0,0]}")
    else:
        print("No specific error message from pgml.last_error_message().")

# --- Verification: Check pgml.projects and pgml.models tables ---
print(f"\n--- Verifying Project '{PROJECT_NAME}' and its Models post-training attempt ---")
project_verify_df = run_sql(f"SELECT id, name, task, algorithm_name as deployed_algorithm FROM pgml.projects WHERE name = '{PROJECT_NAME}';", quiet=True)
project_successfully_created = False # Flag to track actual project creation

if not project_verify_df.empty:
    print(f"‚úÖ Project '{PROJECT_NAME}' found in pgml.projects:")
    print(project_verify_df.to_string())
    project_id = project_verify_df.iloc[0]['id']
    project_successfully_created = True # Set flag
    models_for_project_df = run_sql(f"SELECT id as model_id, algorithm_name, status, metrics, to_char(created_at, 'YYYY-MM-DD HH24:MI:SS') as created_at FROM pgml.models WHERE project_id = {project_id} ORDER BY created_at DESC;", quiet=True)
    if not models_for_project_df.empty:
        print(f"\nModels found in pgml.models for project_id {project_id} ('{PROJECT_NAME}'):")
        print(models_for_project_df.to_string())
    else:
        print(f"‚ö†Ô∏è No models found in pgml.models for project_id {project_id} ('{PROJECT_NAME}'). Training might have failed to produce a model artifact.")
        project_successfully_created = False # Re-evaluate if no models present
else:
    print(f"üî¥ ERROR: Project '{PROJECT_NAME}' NOT found in pgml.projects after training attempt.")
    print("   Review SQL errors from `pgml.train` execution OR `pgml.last_error_message()` output.")
    project_successfully_created = False


# --- Capstone Note on Model Training & Setting MOCK_MODEL_PREDICTIONS ---
print("\n--- Capstone Note on Model Training & Mocking Status ---")
if project_successfully_created:
    print(f"‚úÖ Project '{PROJECT_NAME}' was successfully created/found. Real predictions will be attempted.")
    MOCK_MODEL_PREDICTIONS = False
else:
    print(f"üî¥ NOTE: Project '{PROJECT_NAME}' was NOT successfully created/found in pgml.projects.")
    print("   For this capstone, recommenders will use MOCKED (random) scores.")
    MOCK_MODEL_PREDICTIONS = True
print(f"Python variable MOCK_MODEL_PREDICTIONS set to: {MOCK_MODEL_PREDICTIONS}")

print("\n--- End of Cell 6 ---")

# CELL 7 (Using a custom TYPE for temp_recommendations array)

print(f"--- Creating SQL Function for Real-time Recommendations ('get_recommendations') ---")

if 'PROJECT_NAME' not in globals():
    PROJECT_NAME = 'product_recommender_v1'
    print(f"‚ö†Ô∏è PROJECT_NAME not found globally, using default: '{PROJECT_NAME}'")
print(f"This function will use the trained model: '{PROJECT_NAME}'")

if 'TRAINING_FEATURE_LENGTH' not in globals() or TRAINING_FEATURE_LENGTH is None:
    print("‚ö†Ô∏è TRAINING_FEATURE_LENGTH not found globally from Cell 5, attempting to re-fetch...")
    refetch_length_df = run_sql("SELECT array_length(features, 1) as len FROM training_data WHERE features IS NOT NULL LIMIT 1;", quiet=True)
    if not refetch_length_df.empty and refetch_length_df.iloc[0]['len'] is not None:
        TRAINING_FEATURE_LENGTH = int(refetch_length_df.iloc[0]['len'])
        print(f"   Re-fetched TRAINING_FEATURE_LENGTH: {TRAINING_FEATURE_LENGTH}")
    else:
        TRAINING_FEATURE_LENGTH = 0
        print(f"   üî¥ Could not re-fetch TRAINING_FEATURE_LENGTH. Set to 0.")

if 'MOCK_MODEL_PREDICTIONS' not in globals():
    print("WARNING: MOCK_MODEL_PREDICTIONS not globally set from Cell 6. Checking project existence now.")
    project_exists_check_df = run_sql(f"SELECT 1 FROM pgml.projects WHERE name = '{PROJECT_NAME}';", quiet=True)
    if project_exists_check_df.empty:
        print(f"   Project '{PROJECT_NAME}' not found. Enabling mock mode for get_recommendations.")
        MOCK_MODEL_PREDICTIONS = True
    else:
        print(f"   Project '{PROJECT_NAME}' found. Disabling mock mode for get_recommendations.")
        MOCK_MODEL_PREDICTIONS = False
else:
    print(f"MOCK_MODEL_PREDICTIONS is globally set to: {MOCK_MODEL_PREDICTIONS} (from Cell 6).")

# --- Create a TYPE to match the function's RETURNS TABLE structure ---
type_definition_sql = """
DROP TYPE IF EXISTS recommendation_result_type CASCADE;
CREATE TYPE recommendation_result_type AS (
    product_id INTEGER,
    product_name TEXT,
    category TEXT,
    predicted_score FLOAT
);
"""
run_sql(type_definition_sql, fetch_results=False)
print("‚úÖ Custom type 'recommendation_result_type' created/recreated.")


print("\nDropping existing 'get_recommendations' function signatures...")
run_sql("DROP FUNCTION IF EXISTS get_recommendations(INTEGER, INTEGER);", fetch_results=False, quiet=True)
run_sql("DROP FUNCTION IF EXISTS get_recommendations(INTEGER, INTEGER, BOOLEAN);", fetch_results=False, quiet=True)
print("‚úÖ Attempted to drop existing function signatures.")

get_recommendations_sql = f"""
CREATE OR REPLACE FUNCTION get_recommendations(
    p_user_id INTEGER,
    p_top_n INTEGER DEFAULT 5,
    p_force_mock_predictions BOOLEAN DEFAULT {str(MOCK_MODEL_PREDICTIONS).lower()}
)
RETURNS SETOF recommendation_result_type -- Use the custom type here
AS $$
DECLARE
    v_model_actually_exists BOOLEAN := false;
    v_user_features FLOAT[];
    v_combined_features FLOAT[];
    v_prediction FLOAT;
    r RECORD;
    -- Declare temp_recommendations as an array of our custom type
    temp_recommendations recommendation_result_type[];
    recommendation_row recommendation_result_type; -- Use the custom type for the row variable
BEGIN
    RAISE NOTICE 'get_recommendations: Called for user_id=%, top_n=%, force_mock=%', p_user_id, p_top_n, p_force_mock_predictions;

    SELECT features INTO v_user_features FROM user_features WHERE user_id = p_user_id LIMIT 1;
    IF v_user_features IS NULL OR array_length(v_user_features, 1) = 0 THEN
        RAISE WARNING 'get_recommendations: User % has NULL or empty features. Forcing mock.', p_user_id;
        p_force_mock_predictions := true;
    END IF;

    IF NOT p_force_mock_predictions THEN
        SELECT EXISTS (SELECT 1 FROM pgml.projects WHERE name = '{PROJECT_NAME}') INTO v_model_actually_exists;
        RAISE NOTICE 'get_recommendations: Model "{PROJECT_NAME}" existence check: %', v_model_actually_exists;
    END IF;

    IF NOT p_force_mock_predictions AND v_model_actually_exists THEN
        RAISE NOTICE 'get_recommendations: Attempting REAL predictions with model "{PROJECT_NAME}".';
        FOR r IN (
            SELECT p.product_id as current_pid, p.product_name as current_pname,
                   p.category as current_pcat, pf.features as current_pf_features
            FROM products p JOIN product_features pf ON p.product_id = pf.product_id
            WHERE p.product_id NOT IN (
                    SELECT i.product_id FROM interactions i
                    WHERE i.user_id = p_user_id AND (i.interaction_type = 'purchase' OR i.rating >= 4.0)
                )
                AND v_user_features IS NOT NULL AND array_length(v_user_features, 1) > 0
                AND pf.features IS NOT NULL AND array_length(pf.features, 1) > 0
        ) LOOP
            v_combined_features := NULL; v_prediction := NULL;

            IF r.current_pf_features IS NOT NULL AND array_length(r.current_pf_features, 1) > 0 AND
               v_user_features IS NOT NULL AND array_length(v_user_features, 1) > 0 THEN
                v_combined_features := v_user_features || r.current_pf_features;
                IF array_length(v_combined_features, 1) = ({TRAINING_FEATURE_LENGTH if TRAINING_FEATURE_LENGTH != -1 else 0}) THEN
                    BEGIN
                        RAISE NOTICE 'get_recommendations: Predicting P:% UF_len:% PF_len:% Combined_len:%',
                                     r.current_pid, array_length(v_user_features,1), array_length(r.current_pf_features,1), array_length(v_combined_features,1);
                        v_prediction := pgml.predict('{PROJECT_NAME}', v_combined_features);
                    EXCEPTION
                        WHEN others THEN
                            RAISE WARNING 'get_recommendations: pgml.predict FAILED for P:% - SQLSTATE:%, SQLERRM:%', r.current_pid, SQLSTATE, SQLERRM;
                            v_prediction := NULL;
                    END;
                ELSE
                    RAISE WARNING 'get_recommendations: Bad feature vector length for P:% U:%. UF_len:% PF_len:% Combined_len:% Expected:%',
                                 r.current_pid, p_user_id, array_length(v_user_features,1), array_length(r.current_pf_features,1),
                                 array_length(v_combined_features,1), ({TRAINING_FEATURE_LENGTH if TRAINING_FEATURE_LENGTH != -1 else 0});
                END IF;
            END IF;

            IF v_prediction IS NOT NULL THEN
                 recommendation_row := (r.current_pid, r.current_pname, r.current_pcat, v_prediction::FLOAT)::recommendation_result_type;
                 temp_recommendations := array_append(temp_recommendations, recommendation_row);
            END IF;
        END LOOP;

        IF array_length(temp_recommendations, 1) > 0 THEN
            -- Need to unnest and then order and limit.
            -- Create a temporary table for sorting, then return from it.
            DROP TABLE IF EXISTS __temp_recs_for_user;
            CREATE TEMP TABLE __temp_recs_for_user AS
            SELECT * FROM unnest(temp_recommendations);

            RETURN QUERY SELECT tr.product_id, tr.product_name, tr.category, tr.predicted_score
                         FROM __temp_recs_for_user tr
                         ORDER BY tr.predicted_score DESC NULLS LAST LIMIT p_top_n;
        ELSE
             RAISE NOTICE 'get_recommendations: No valid predictions made in real mode.';
        END IF;

    ELSE
        IF p_force_mock_predictions THEN
            RAISE NOTICE 'get_recommendations: Mock mode: Generating random scores for user %.', p_user_id;
        ELSE
             RAISE NOTICE 'get_recommendations: Model "{PROJECT_NAME}" not found: Generating random scores for user %.', p_user_id;
        END IF;

        RETURN QUERY
        SELECT p.product_id, p.product_name, p.category, (random() * 2.0 + 2.5)::FLOAT
        FROM products p
        WHERE p.product_id NOT IN (
              SELECT i.product_id FROM interactions i
              WHERE i.user_id = p_user_id AND (i.interaction_type = 'purchase' OR i.rating >= 4.0)
          )
        ORDER BY random() LIMIT p_top_n; -- Random order for mock if scores are all random
    END IF;
END;
$$ LANGUAGE plpgsql;
"""
run_sql(type_definition_sql, fetch_results=False) # Ensure type is created before function
run_sql(get_recommendations_sql, fetch_results=False)
print(f"‚úÖ SQL function 'get_recommendations' created/recreated (using custom type for temp array).")

# --- Test the SQL Recommendation Function ---
# (The test block remains the same)
print("\n--- Testing 'get_recommendations' SQL Function ---")
test_user_id = 1
test_top_n = 5
print(f"Fetching top {test_top_n} recommendations for user_id: {test_user_id} (Python MOCK_MODEL_PREDICTIONS var is: {MOCK_MODEL_PREDICTIONS})")

if 'NUM_USERS' in globals() and (test_user_id > NUM_USERS or test_user_id <=0):
    print(f"‚ö†Ô∏è Test user ID {test_user_id} is out of range (1-{NUM_USERS}). Using User 1 if NUM_USERS > 0 else skipping test.")
    if NUM_USERS > 0: test_user_id = 1
    else: test_user_id = None

if test_user_id is not None:
    recommendations_df = run_sql(f"SELECT * FROM get_recommendations(p_user_id => {test_user_id}, p_top_n => {test_top_n});")
    if not recommendations_df.empty:
        print(f"\nRecommendations for User ID {test_user_id}:")
        print(recommendations_df.to_string())
    else:
        print(f"‚ö†Ô∏è No recommendations returned for user_id {test_user_id}.")
        print(f"   Check RAISE NOTICE/WARNING messages. Current MOCK_MODEL_PREDICTIONS={MOCK_MODEL_PREDICTIONS}")
else:
    print("Skipping get_recommendations test as test_user_id is invalid or NUM_USERS is 0.")

print("\n--- End of Cell 7 ---")

# CELL 8: API DESIGN WITH FASTAPI
#@markdown This cell presents the Python code for a FastAPI application that would serve recommendations.
#@markdown This code is for design and understanding; **it will not be run as a live server in this Colab notebook.**
#@markdown You would typically save this to a `main.py` file and run it with `uvicorn main:app --reload`.

print("--- Designing the FastAPI Application for Recommendations ---")

#@markdown #### Connection Pooling (as per Part 3, Slide 2)
#@markdown Using a connection pool is crucial for performance in a web application.
connection_pool_code = """
# main.py (Illustrative - For local execution with Uvicorn)
import os
from fastapi import FastAPI, HTTPException
import psycopg2
from psycopg2.pool import SimpleConnectionPool

# --- Configuration ---
# In a real app, use environment variables for sensitive info
DATABASE_URL = os.environ.get("DATABASE_URL", "your_postgres_connection_string_here_from_cell_1")
# It's critical that this DATABASE_URL matches your actual DB_CONNECTION_STRING from Cell 1

# Initialize a connection pool
try:
    pool = SimpleConnectionPool(
        minconn=2,  # Minimum number of connections in the pool
        maxconn=10, # Maximum number of connections
        dsn=DATABASE_URL
    )
    print("Database connection pool initialized.")
except Exception as e:
    print(f"FATAL: Could not initialize database connection pool: {e}")
    pool = None # Ensure pool is None if initialization fails

app = FastAPI(title="PostgresML Recommender API")

@app.on_event("startup")
async def startup_event():
    if pool is None:
        # This will prevent the app from starting if pool is not available
        raise RuntimeError("Database connection pool is not available. FastAPI app cannot start.")
    print("FastAPI application started successfully with DB pool.")

@app.on_event("shutdown")
def shutdown_event():
    if pool:
        pool.closeall()
        print("Database connection pool closed.")

# --- API Endpoint (as per Part 5, Slide 2) ---
@app.get("/recommendations/{user_id}", tags=["Recommendations"])
async def get_user_recommendations_api(user_id: int, top_n: int = 5):
    '''
    Fetches top N product recommendations for a given user_id.
    '''
    if pool is None:
        raise HTTPException(status_code=503, detail="Database service unavailable")

    conn = None
    try:
        conn = pool.getconn() # Get a connection from the pool
        with conn.cursor() as cur:
            # Call the SQL function created in Cell 7
            cur.execute("SELECT product_id, product_name, category, predicted_score FROM get_recommendations(%s, %s);", (user_id, top_n))
            recs_data = cur.fetchall()

            if not recs_data:
                # Return 404 if user exists but no recommendations, or handle as appropriate
                # Check if user exists to differentiate
                cur.execute("SELECT 1 FROM users WHERE user_id = %s;", (user_id,))
                if not cur.fetchone():
                    raise HTTPException(status_code=404, detail=f"User ID {user_id} not found.")
                return {"user_id": user_id, "recommendations": []} # User exists, no recs

            recommendations = [
                {"product_id": r[0], "product_name": r[1], "category": r[2], "predicted_score": round(r[3], 4)}
                for r in recs_data
            ]
            return {"user_id": user_id, "top_n_requested": top_n, "recommendations": recommendations}
    except psycopg2.Error as db_err:
        # Log the database error, return a generic server error
        print(f"Database error for user_id {user_id}: {db_err}")
        raise HTTPException(status_code=500, detail="Internal database error")
    except Exception as e:
        # Log other errors
        print(f"An unexpected error occurred for user_id {user_id}: {e}")
        raise HTTPException(status_code=500, detail="An unexpected internal server error occurred")
    finally:
        if conn:
            pool.putconn(conn) # Return the connection to the pool

# To run this (locally, not in Colab):
# 1. Save this code as `main.py`.
# 2. Set the DATABASE_URL environment variable: `export DATABASE_URL="your_actual_connection_string"`
# 3. Run Uvicorn: `uvicorn main:app --reload --port 8000`
# 4. Access in browser or with curl: `curl http://localhost:8000/recommendations/1?top_n=3`
"""

print("--- FastAPI Application Code (for main.py) ---")
# Display the code using IPython.display.Code for better formatting in Colab
from IPython.display import Code
Code(connection_pool_code, language='python')

print("\nTo make this API live:")
print("1. Replace 'your_postgres_connection_string_here_from_cell_1' with your actual DB_CONNECTION_STRING from Cell 1 (or better, use an environment variable).")
print("2. Save the Python code above into a file named `main.py`.")
print("3. Install FastAPI and Uvicorn: `pip install fastapi uvicorn[standard]`")
print("4. Run from your terminal: `uvicorn main:app --reload --port 8000`")
print("5. You can then test it, for example, with a curl command in your terminal:")
print(f"   `curl \"http://localhost:8000/recommendations/{test_user_id}?top_n=3\"` (replace {test_user_id} if needed)")
print("   Or open `http://localhost:8000/docs` in your browser for interactive API documentation (Swagger UI).")

print("\n--- End of Cell 8 ---")
# Display the FastAPI code block so it's easily copyable
display(Code(connection_pool_code, language='python'))

# CELL 9: PRODUCTION CONSIDERATIONS - MONITORING & SECURITY (SQL)
#@markdown Implements basic SQL constructs for monitoring predictions and setting up database permissions.
print("--- Production Considerations: Basic Monitoring & Security ---")

# --- Basic Monitoring (as per Part 4, Slide 1) ---
print("\nSetting up basic monitoring (schema, table, view)...")
run_sql("CREATE SCHEMA IF NOT EXISTS monitoring;", fetch_results=False)
monitoring_table_sql = """
CREATE TABLE IF NOT EXISTS monitoring.predictions (
    id SERIAL PRIMARY KEY, model_name TEXT, user_id_requested INTEGER,
    num_recommendations_generated INTEGER, first_recommendation_score FLOAT,
    response_time_ms FLOAT, timestamp TIMESTAMP DEFAULT NOW()
);"""
run_sql(monitoring_table_sql, fetch_results=False)
print("‚úÖ Table 'monitoring.predictions' created (if not exists).")

monitoring_view_sql = """
CREATE OR REPLACE VIEW monitoring.metrics_view AS
SELECT model_name, COUNT(*) as total_prediction_calls,
       AVG(response_time_ms) as avg_response_time_ms,
       AVG(num_recommendations_generated) as avg_recs_generated,
       MIN(timestamp) as first_call_at, MAX(timestamp) as last_call_at
FROM monitoring.predictions GROUP BY model_name;"""
run_sql(monitoring_view_sql, fetch_results=False)
print("‚úÖ View 'monitoring.metrics_view' created/recreated.")

print("\nSimulating logging of prediction calls to 'monitoring.predictions'...")
log_sim_sql = "INSERT INTO monitoring.predictions (model_name, user_id_requested, num_recommendations_generated, first_recommendation_score, response_time_ms) VALUES (%s, %s, %s, %s, %s);"
# Fetch a sample score IF model worked and mock is False, otherwise use a dummy score for logging
sample_recs_for_log_df = pd.DataFrame() # Initialize
if 'MOCK_MODEL_PREDICTIONS' in globals() and not MOCK_MODEL_PREDICTIONS and 'test_user_id' in globals():
    sample_recs_for_log_df = run_sql(f"SELECT predicted_score FROM get_recommendations({test_user_id}, 1) LIMIT 1;", quiet=True)

top_score_for_log = None
if not sample_recs_for_log_df.empty and 'predicted_score' in sample_recs_for_log_df.columns and sample_recs_for_log_df.iloc[0]['predicted_score'] is not None:
    top_score_for_log = sample_recs_for_log_df.iloc[0]['predicted_score']
else: # Fallback if no real score (mock mode or error)
    top_score_for_log = round(random.uniform(2.5, 4.5), 2) if 'random' in globals() else 3.5


dummy_logs_data = [
    (PROJECT_NAME if 'PROJECT_NAME' in globals() else 'product_recommender_v1', test_user_id if 'test_user_id' in globals() else 1, 5, top_score_for_log, random.uniform(50, 200)),
    (PROJECT_NAME if 'PROJECT_NAME' in globals() else 'product_recommender_v1', random.randint(1, NUM_USERS if 'NUM_USERS' in globals() else 100), 3, round(random.uniform(1,5),2), random.uniform(40, 150)),
    (PROJECT_NAME if 'PROJECT_NAME' in globals() else 'product_recommender_v1', random.randint(1, NUM_USERS if 'NUM_USERS' in globals() else 100), 5, round(random.uniform(1,5),2), random.uniform(60, 250))
]
for log_entry in dummy_logs_data: run_sql(log_sim_sql, log_entry, fetch_results=False, quiet=True)
print(f"‚úÖ Inserted {len(dummy_logs_data)} dummy log entries.")

print("\n--- Current Monitoring Metrics ---")
metrics_df = run_sql("SELECT * FROM monitoring.metrics_view;")
if not metrics_df.empty: print(metrics_df.to_string())
else: print("No metrics available yet in 'monitoring.metrics_view'.")


# --- Essential Security (as per Part 4, Slide 2 - MODIFIED) ---
API_ROLE_NAME = "recommender_api_user" #@param {type:"string"}
print(f"\n--- Configuring Permissions for hypothetical role: '{API_ROLE_NAME}' ---")
print(f"NOTE: Actual 'CREATE ROLE {API_ROLE_NAME};' might be restricted in this environment.")
print(f"       We will proceed to show the GRANT statements that *would* be applied.")

# Attempt to create role, but expect it might fail due to permissions.
# We won't stop the script if it fails, just note it.
print(f"\nAttempting to create role '{API_ROLE_NAME}' (may fail due to DB user permissions)...")
try:
    # Try dropping first in case it exists from a run where user HAD perms, to avoid "already exists"
    run_sql(f"DROP ROLE IF EXISTS {API_ROLE_NAME};", fetch_results=False, quiet=True)
    run_sql(f"CREATE ROLE {API_ROLE_NAME};", fetch_results=False, quiet=False) # Not quiet to see error
    print(f"‚úÖ INFO: 'CREATE ROLE {API_ROLE_NAME}' statement executed. If no error above, it was successful. If 'permission denied', the role was not created by this script.")
except Exception as e_create_role: # run_sql already prints, but this is an extra catch
    print(f"‚ÑπÔ∏è INFO: Could not create role '{API_ROLE_NAME}' due to: {e_create_role}. This is expected if current DB user lacks CREATEROLE privilege.")

# Grant necessary permissions for the get_recommendations function
# Corrected signature for get_recommendations: (INTEGER, INTEGER, BOOLEAN)
grant_execute_sql = f"GRANT EXECUTE ON FUNCTION get_recommendations(INTEGER, INTEGER, BOOLEAN) TO {API_ROLE_NAME};"
print(f"\nAttempting: {grant_execute_sql}")
try:
    run_sql(grant_execute_sql, fetch_results=False, quiet=False) # Not quiet
    print(f"‚úÖ Statement 'GRANT EXECUTE ON get_recommendations(INTEGER, INTEGER, BOOLEAN)' sent. If role '{API_ROLE_NAME}' exists and current user has grant option, this should succeed.")
except Exception as e_grant_func:
    print(f"‚ÑπÔ∏è INFO: Could not grant EXECUTE on function: {e_grant_func}. Role might not exist or current user lacks grant option.")


print(f"\nAttempting to GRANT SELECT on necessary tables to '{API_ROLE_NAME}'...")
tables_for_api_select = ["users", "products", "user_features", "product_features", "interactions"]
all_grants_successful = True
for table_name in tables_for_api_select:
    grant_select_sql = f"GRANT SELECT ON TABLE {table_name} TO {API_ROLE_NAME};"
    try:
        run_sql(grant_select_sql, fetch_results=False, quiet=True) # Quiet for loop
    except Exception as e_grant_table:
        print(f"‚ÑπÔ∏è INFO: Could not grant SELECT on {table_name} to {API_ROLE_NAME}: {e_grant_table}")
        all_grants_successful = False
if all_grants_successful:
    print(f"‚úÖ Statements to GRANT SELECT on tables ({', '.join(tables_for_api_select)}) sent.")

print(f"\nAttempting to GRANT USAGE on schema public to '{API_ROLE_NAME}'...")
try:
    run_sql(f"GRANT USAGE ON SCHEMA public TO {API_ROLE_NAME};", fetch_results=False, quiet=True)
    print(f"‚úÖ Statement 'GRANT USAGE ON SCHEMA public' sent.")
except Exception as e_grant_schema:
     print(f"‚ÑπÔ∏è INFO: Could not grant USAGE on schema public: {e_grant_schema}")


print(f"\nSecurity Configuration Summary for role '{API_ROLE_NAME}':")
print(f"- The above GRANT statements define the necessary permissions.")
print(f"- If role creation failed due to permissions, these grants would also fail or have no effect until the role is manually created by an admin.")
print(f"- For API connection: role '{API_ROLE_NAME}' would need LOGIN privilege and a password, or other authentication method.")

print("\n--- End of Cell 9 ---")

# CELL 10: CAPSTONE CONCLUSION & NEXT STEPS
#@markdown Summarizes the project, its alignment with course objectives, and potential future work.
print("="*80)
print("CAPSTONE PROJECT CONCLUSION: End-to-End Product Recommender with PostgresML")
print("="*80)

print("\nüéâ Congratulations on completing the Product Recommender Capstone Project! üéâ")

print("\n--- Summary of Achievements ---")
print("This project successfully demonstrated an end-to-end workflow for building a recommendation system using PostgresML, aligning with the course content:")
print("1.  **Data Foundation:** Generated and structured synthetic data for users, products, and their interactions within PostgreSQL.")
print("2.  **Feature Engineering in SQL:** Created numerical feature arrays for users and products, preparing data for machine learning.")
print("3.  **In-Database Model Training:** Leveraged `pgml.train` to train a regression model (`product_recommender_v1`) directly on data residing in PostgreSQL, predicting user preferences (ratings).")
print("4.  **Real-time Recommendation Logic:** Developed a PL/pgSQL function (`get_recommendations`) that uses the trained model via `pgml.predict` to score products for a given user and return top recommendations.")
print("5.  **API Design for Integration:** Designed a FastAPI application to expose the recommendation logic as a consumable web service, including considerations for database connection pooling.")
print("6.  **Basic Production Considerations:** Implemented foundational SQL structures for monitoring prediction requests and for database security (role-based access control).")

print("\n--- Alignment with Course Objectives & Capstone Goals ---")
print("This project directly applied key concepts from the course:")
print("- **Core PostgresML:** Utilized `pgml.train` for training and `pgml.predict` for inference.")
print("- **Feature Engineering:** Transformed raw data into `FLOAT[]` features suitable for the ML model.")
print("- **Model Deployment & Integration (Design):** While not running a live server in Colab, the FastAPI code and SQL function represent the integration of the ML model into an application layer.")
print("- **Production Insights:** The monitoring table/view and security role setup illustrate initial steps towards production readiness.")
print("- **Interactive Application (Conceptual):** The `get_recommendations` function serves as the backend for an interactive recommendation application.")
print("\nWhile a full-scale production deployment with live A/B testing, a sophisticated UI dashboard, and fully automated retraining pipelines is beyond the scope of this single notebook exercise, this capstone establishes the fundamental building blocks and demonstrates the power of PostgresML as an MLOps platform within the database.")

print("\n--- Potential Future Enhancements & Real-World Extensions ---")
print("- **Advanced Feature Engineering:** Incorporate text embeddings (e.g., from product descriptions using `pgml.embed` if available or `pgml.transform` if a suitable model is enabled) or image features.")
print("- **More Sophisticated Models:** Explore other algorithms supported by `pgml.train` (e.g., classification for 'did purchase' vs 'did not', or ranking models if available). Investigate different regression algorithms.")
print("- **Implicit Feedback:** Adapt the model to use implicit feedback (views, add-to-carts) if explicit ratings are sparse.")
print("- **Cold Start Problem:** Implement strategies for new users or new items (e.g., content-based recommendations as a fallback).")
print("- **Full API Deployment:** Deploy the FastAPI application to a cloud service (e.g., AWS Lambda, Google Cloud Run, Heroku).")
print("- **Comprehensive Monitoring:** Integrate with tools like Prometheus/Grafana for richer monitoring dashboards.")
print("- **Automated Retraining:** Use `pg_cron` or an external orchestrator (Airflow, Kubeflow Pipelines) to schedule `pgml.train` with updated data.")
print("- **A/B Testing Framework:** Implement a system to serve different recommendation models to segments of users and compare their performance.")

print("\nThis capstone project provides a solid foundation and understanding of how to build and integrate machine learning models for recommendation tasks directly within a PostgreSQL environment using PostgresML. Well done!")
print("="*80)

# CELL 11: CLEANUP (OPTIONAL)
#@markdown Drops all tables, functions, roles, and schemas created during this capstone project.
#@markdown **Check the box and run this cell to perform cleanup.**
print("--- Optional Database Cleanup ---")
perform_cleanup = False #@param {type:"boolean"}

if perform_cleanup:
    print("\nüßπ Performing cleanup as requested...")

    # Functions
    print("  Dropping SQL function 'get_recommendations'...")
    run_sql("DROP FUNCTION IF EXISTS get_recommendations(INTEGER, INTEGER);", fetch_results=False, quiet=True)
    print("  Dropping SQL function 'process_features' (if created from course slides)...")
    run_sql("DROP FUNCTION IF EXISTS process_features(JSONB);", fetch_results=False, quiet=True)

    # Monitoring objects (view first, then table, then schema)
    print("  Dropping monitoring objects...")
    run_sql("DROP VIEW IF EXISTS monitoring.metrics_view CASCADE;", fetch_results=False, quiet=True)
    run_sql("DROP TABLE IF EXISTS monitoring.predictions CASCADE;", fetch_results=False, quiet=True)
    run_sql("DROP SCHEMA IF EXISTS monitoring CASCADE;", fetch_results=False, quiet=True)

    # Core data and feature tables (order by dependency or use CASCADE)
    print("  Dropping data and feature tables...")
    run_sql("DROP TABLE IF EXISTS training_data CASCADE;", fetch_results=False, quiet=True)
    run_sql("DROP TABLE IF EXISTS interactions CASCADE;", fetch_results=False, quiet=True)
    run_sql("DROP TABLE IF EXISTS user_features CASCADE;", fetch_results=False, quiet=True)
    run_sql("DROP TABLE IF EXISTS product_features CASCADE;", fetch_results=False, quiet=True)
    run_sql("DROP TABLE IF EXISTS users CASCADE;", fetch_results=False, quiet=True)
    run_sql("DROP TABLE IF EXISTS products CASCADE;", fetch_results=False, quiet=True)

    # Roles (ensure role doesn't own objects or have dependencies not covered by CASCADE on tables/functions)
    API_ROLE_NAME_CLEANUP = "recommender_api_user" # Ensure this matches the role name used
    print(f"  Dropping role '{API_ROLE_NAME_CLEANUP}'...")
    try:
        run_sql(f"DROP ROLE IF EXISTS {API_ROLE_NAME_CLEANUP};", fetch_results=False, quiet=True)
    except Exception as e_role_final_drop:
        print(f"  ‚ö†Ô∏è Note: Could not drop role '{API_ROLE_NAME_CLEANUP}' directly. It might require manual intervention if it still has privileges or owns objects. Error: {e_role_final_drop}")


    # PGML Project (Optional - pgml.train creates a project)
    # `pgml.delete_project` or `pgml.archive_project` might be needed if you want to fully remove it.
    # This is less critical for a demo unless project names clash.
    # Example: run_sql(f"SELECT pgml.archive_project('{PROJECT_NAME}');", fetch_results=False, quiet=True)

    print("\n‚úÖ Cleanup operations attempted. Please verify in your database client if needed.")
else:
    print("\nCleanup skipped as 'perform_cleanup' checkbox is not checked.")

print("\n--- End of Cleanup Cell ---")