{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# CELL 0: CAPSTONE PROJECT - END-TO-END PRODUCT RECOMMENDER\n",
        "#@markdown # Capstone: Building & \"Deploying\" an End-to-End Product Recommender with PostgresML\n",
        "#@markdown ## Project Goal:\n",
        "#@markdown This project walks through the creation of a product recommendation system using PostgresML. We will cover:\n",
        "#@markdown 1. Generating synthetic data for users, products, and interactions.\n",
        "#@markdown 2. Engineering features for users and products.\n",
        "#@markdown 3. Preparing training data for a recommendation model.\n",
        "#@markdown 4. Training a regression-based recommendation model using `pgml.train`.\n",
        "#@markdown 5. Creating a SQL function to get recommendations using `pgml.predict`.\n",
        "#@markdown 6. Designing a FastAPI endpoint to serve these recommendations.\n",
        "#@markdown 7. Implementing basic SQL-based monitoring and security constructs.\n",
        "#@markdown\n",
        "#@markdown ## Learning Objectives Alignment:\n",
        "#@markdown This capstone aims to apply concepts of data preparation, feature engineering, model training (`pgml.train`), model serving (`pgml.predict` via SQL functions), API integration (FastAPI design), and basic production considerations (monitoring, security) as covered in the course.\n",
        "#@markdown ---\n",
        "#@markdown ### Instructions:\n",
        "#@markdown 1. **Set Connection String:** Ensure `DB_CONNECTION_STRING` in Cell 1 is correct.\n",
        "#@markdown 2. **Run Cells Sequentially:** Execute each cell in order.\n",
        "#@markdown 3. **Observe Outputs:** Pay attention to print statements and DataFrame outputs.\n",
        "#@markdown ---\n",
        "\n",
        "import IPython\n",
        "display(IPython.display.Markdown(\"## Let's Build an End-to-End Product Recommendation System!\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "SG-MCZA2KeRB",
        "outputId": "f70ec4c9-ba54-4ae2-b468-a49162650138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Let's Build an End-to-End Product Recommendation System!"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 1: CONFIGURATION - DATABASE CONNECTION STRING\n",
        "#@markdown ### PostgreSQL Connection String\n",
        "#@markdown Enter your full PostgresML connection string below.\n",
        "DB_CONNECTION_STRING = \"postgres://u_zvtt5tif9x1yz0l:xjeuxoo9levuxmm@437a9d42-c398-4c00-9906-c9b5fc2e7d61.gcp.db.postgresml.org:6432/pgml_dhrhajumjl7tmkm\" #@param {type:\"string\"}\n",
        "\n",
        "# --- Sanity check for the connection string ---\n",
        "if 'DB_CONNECTION_STRING' not in globals() or not DB_CONNECTION_STRING or \"postgres://\" not in DB_CONNECTION_STRING:\n",
        "    print(\"🔴 ERROR: DB_CONNECTION_STRING is not set or does not look like a valid PostgreSQL connection string.\")\n",
        "    print(\"Please set it correctly in this cell before proceeding.\")\n",
        "    # raise ValueError(\"DB_CONNECTION_STRING not configured properly.\") # Optional: Stop execution\n",
        "else:\n",
        "    try:\n",
        "        user_part = DB_CONNECTION_STRING.split(\"://\")[1].split(\":\")[0]\n",
        "        host_part = DB_CONNECTION_STRING.split(\"@\")[1]\n",
        "        masked_conn_string = f\"postgres://{user_part}:********@{host_part}\"\n",
        "    except Exception:\n",
        "        masked_conn_string = \"postgres://USER:********@HOST/DATABASE (Could not fully parse to mask)\"\n",
        "    print(f\"✅ DB_CONNECTION_STRING is set (password masked for display): {masked_conn_string}\")\n",
        "    print(\"You can now run the next cell to install dependencies and define helpers.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iarH_Y7dKlLB",
        "outputId": "e5500563-030d-4683-abfe-3db447635850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DB_CONNECTION_STRING is set (password masked for display): postgres://u_zvtt5tif9x1yz0l:********@437a9d42-c398-4c00-9906-c9b5fc2e7d61.gcp.db.postgresml.org:6432/pgml_dhrhajumjl7tmkm\n",
            "You can now run the next cell to install dependencies and define helpers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2: INSTALL DEPENDENCIES & DEFINE HELPER FUNCTIONS\n",
        "!pip install psycopg2-binary Faker pandas -q\n",
        "!apt-get update -qq && apt-get install -y postgresql-client -qq\n",
        "\n",
        "import psycopg2\n",
        "import json\n",
        "import pandas as pd\n",
        "from faker import Faker\n",
        "import textwrap\n",
        "import time\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "import sys # For flushing output\n",
        "\n",
        "# --- Enhanced Helper function to run SQL commands ---\n",
        "def run_sql(sql_query, params=None, fetch_results=True, conn_string=None, quiet=False):\n",
        "    if not quiet:\n",
        "        print(f\" Kicking off run_sql\")\n",
        "        sys.stdout.flush()\n",
        "    if conn_string is None: conn_string = DB_CONNECTION_STRING\n",
        "    if not conn_string:\n",
        "        if not quiet: print(\"🔴 run_sql: DB_CONNECTION_STRING is not available.\")\n",
        "        return pd.DataFrame()\n",
        "    conn = None\n",
        "    try:\n",
        "        if not quiet: print(f\"  Connecting to DB...\")\n",
        "        sys.stdout.flush()\n",
        "        conn = psycopg2.connect(conn_string)\n",
        "        # No pgvector needed for this specific capstone version\n",
        "        if not quiet: print(f\"  Connection successful. Executing query...\")\n",
        "        sys.stdout.flush()\n",
        "        with conn.cursor() as cur:\n",
        "            cur.execute(sql_query, params)\n",
        "            if not quiet: print(f\"  Query executed.\")\n",
        "            sys.stdout.flush()\n",
        "            if fetch_results and cur.description:\n",
        "                columns = [desc[0] for desc in cur.description]\n",
        "                results_data = cur.fetchall() # <<<< ERROR MIGHT HAPPEN HERE if function errors\n",
        "                if not quiet: print(f\"  Fetched {len(results_data)} rows.\")\n",
        "                sys.stdout.flush()\n",
        "                return pd.DataFrame(results_data, columns=columns) if results_data else pd.DataFrame(columns=columns)\n",
        "            conn.commit() # Commit for DDL/DML if fetch_results is false or no description\n",
        "            if not quiet: print(f\"  Committed (if applicable).\")\n",
        "            sys.stdout.flush()\n",
        "        return pd.DataFrame()\n",
        "    except psycopg2.Error as e:\n",
        "        if not quiet:\n",
        "            print(f\"🔴 SQL Error in run_sql:\")\n",
        "            print(f\"   Query: {sql_query[:500]}{'...' if len(sql_query) > 500 else ''}\")\n",
        "            if params: print(f\"   Params: {params}\")\n",
        "            print(f\"   Error Code: {e.pgcode}\") # Standard error code\n",
        "            print(f\"   Raw pgerror: {e.pgerror}\") # Raw error message from DB\n",
        "            # Try to get more detailed diagnostic message if available\n",
        "            if hasattr(e, 'diag') and e.diag and hasattr(e.diag, 'message_primary'):\n",
        "                print(f\"   Primary Message: {e.diag.message_primary}\")\n",
        "            if hasattr(e, 'diag') and e.diag and hasattr(e.diag, 'message_detail'):\n",
        "                print(f\"   Detail Message: {e.diag.message_detail}\")\n",
        "            if hasattr(e, 'diag') and e.diag and hasattr(e.diag, 'message_hint'):\n",
        "                print(f\"   Hint: {e.diag.message_hint}\")\n",
        "            print(f\"   Full Python Error: {e}\")\n",
        "            sys.stdout.flush()\n",
        "        if conn: conn.rollback()\n",
        "    except Exception as ex:\n",
        "        if not quiet: print(f\"🔴 Non-SQL Error in run_sql: {ex}\"); sys.stdout.flush()\n",
        "    finally:\n",
        "        if conn:\n",
        "            conn.close()\n",
        "        # Corrected indentation for this block\n",
        "        if not quiet: print(f\"  Database connection closed.\"); sys.stdout.flush()\n",
        "    return pd.DataFrame()\n",
        "\n",
        "def print_wrapped(text, width=100): print(textwrap.fill(str(text), width=width))\n",
        "print(\"\\n✅ Dependencies installed and helper functions defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcJ6VBj6PIGB",
        "outputId": "7146cc59-8773-4341-e7f0-ee5b42190799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\n",
            "✅ Dependencies installed and helper functions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3: VERIFY POSTGRESML EXTENSION & GET VERSION\n",
        "print(\"--- Verifying pgml Extension and Getting Version ---\")\n",
        "\n",
        "# Check for pgml\n",
        "print(\"\\nChecking for 'pgml' extension...\")\n",
        "pgml_check_sql = \"SELECT extname FROM pg_extension WHERE extname = 'pgml';\"\n",
        "pgml_df = run_sql(pgml_check_sql, quiet=True)\n",
        "if not pgml_df.empty and 'pgml' in pgml_df['extname'].values:\n",
        "    print(\"✅ 'pgml' extension is enabled.\")\n",
        "else:\n",
        "    print(\"⚠️ 'pgml' extension not found. Attempting to enable...\")\n",
        "    run_sql(\"CREATE EXTENSION IF NOT EXISTS pgml;\", fetch_results=False, quiet=True)\n",
        "    time.sleep(1)\n",
        "    pgml_df = run_sql(pgml_check_sql, quiet=True)\n",
        "    if not pgml_df.empty and 'pgml' in pgml_df['extname'].values:\n",
        "         print(\"✅ 'pgml' extension successfully enabled.\")\n",
        "    else:\n",
        "        print(\"🔴 ERROR: Failed to enable or find 'pgml' extension. This is required.\")\n",
        "\n",
        "# Get PostgresML version\n",
        "print(\"\\n--- Attempting to get PostgresML version ---\")\n",
        "COURSE_PGML_VERSION = \"Unknown\"\n",
        "try:\n",
        "    version_df = run_sql(\"SELECT pgml.version();\", quiet=True)\n",
        "    if not version_df.empty:\n",
        "        COURSE_PGML_VERSION = version_df.iloc[0][0]\n",
        "        print(f\"✅ PostgresML Version: {COURSE_PGML_VERSION}\")\n",
        "    else:\n",
        "        print(\"⚠️ pgml.version() did not return results.\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Could not get pgml.version(): {e}\")\n",
        "\n",
        "print(\"\\n--- End of Cell 3 ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ2gPQTjPZYP",
        "outputId": "795af99b-4bed-4187-9d76-c92ebed3244d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Verifying pgml Extension and Getting Version ---\n",
            "\n",
            "Checking for 'pgml' extension...\n",
            "✅ 'pgml' extension is enabled.\n",
            "\n",
            "--- Attempting to get PostgresML version ---\n",
            "✅ PostgresML Version: 2.9.3 (46acfb09ec2001b94e6f1e4a506e97c20d73a4d5)\n",
            "\n",
            "--- End of Cell 3 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-f6ee60886454>:26: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  COURSE_PGML_VERSION = version_df.iloc[0][0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4: DATA GENERATION & SCHEMA SETUP\n",
        "#@markdown Generates synthetic data for users, products, and interactions, then creates the necessary tables.\n",
        "print(\"--- Generating Synthetic Data & Setting Up Schema ---\")\n",
        "\n",
        "from faker import Faker\n",
        "fake = Faker()\n",
        "Faker.seed(0) # for reproducibility\n",
        "random.seed(0)\n",
        "\n",
        "#@markdown #### Data Generation Parameters:\n",
        "NUM_USERS = 150 #@param {type:\"integer\"}\n",
        "NUM_PRODUCTS = 70 #@param {type:\"integer\"}\n",
        "NUM_INTERACTIONS = 750 #@param {type:\"integer\"}\n",
        "\n",
        "print(f\"Parameters: {NUM_USERS} users, {NUM_PRODUCTS} products, {NUM_INTERACTIONS} interactions.\")\n",
        "\n",
        "# --- Cleanup existing tables (order matters for FKs, or use CASCADE) ---\n",
        "print(\"\\nDropping existing tables (if any)...\")\n",
        "tables_to_drop = [\n",
        "    \"monitoring.predictions\", \"monitoring.metrics_view\",\n",
        "    \"training_data\", \"interactions\",\n",
        "    \"user_features\", \"product_features\",\n",
        "    \"users\", \"products\"\n",
        "]\n",
        "for table_name in tables_to_drop:\n",
        "    if table_name.endswith(\"_view\"):\n",
        "        run_sql(f\"DROP VIEW IF EXISTS {table_name} CASCADE;\", fetch_results=False, quiet=True)\n",
        "    else:\n",
        "        run_sql(f\"DROP TABLE IF EXISTS {table_name} CASCADE;\", fetch_results=False, quiet=True)\n",
        "run_sql(\"DROP SCHEMA IF EXISTS monitoring CASCADE;\", fetch_results=False, quiet=True)\n",
        "print(\"✅ Dropped existing project-related tables and monitoring schema.\")\n",
        "\n",
        "# --- Create Tables ---\n",
        "print(\"\\nCreating database tables...\")\n",
        "run_sql(\"CREATE TABLE users (user_id INTEGER PRIMARY KEY, age INTEGER, gender TEXT, country_code TEXT);\", fetch_results=False)\n",
        "# ADDED product_name to products table\n",
        "run_sql(\"CREATE TABLE products (product_id INTEGER PRIMARY KEY, product_name TEXT, category TEXT, brand TEXT, price FLOAT);\", fetch_results=False)\n",
        "run_sql(\"\"\"\n",
        "    CREATE TABLE interactions (\n",
        "        interaction_id SERIAL PRIMARY KEY,\n",
        "        user_id INTEGER REFERENCES users(user_id),\n",
        "        product_id INTEGER REFERENCES products(product_id),\n",
        "        interaction_type TEXT,\n",
        "        rating FLOAT,\n",
        "        timestamp TIMESTAMP DEFAULT NOW()\n",
        "    );\"\"\", fetch_results=False)\n",
        "print(\"✅ Core tables created: users, products, interactions.\")\n",
        "\n",
        "# --- Populate Users ---\n",
        "print(\"\\nPopulating 'users' table...\")\n",
        "user_data = [(i, random.randint(18, 70), random.choice(['Male', 'Female', 'Other']), fake.country_code()) for i in range(1, NUM_USERS + 1)]\n",
        "for u_data in user_data: run_sql(\"INSERT INTO users VALUES (%s, %s, %s, %s);\", u_data, fetch_results=False, quiet=True)\n",
        "print(f\"✅ {len(user_data)} users inserted.\")\n",
        "\n",
        "# --- Populate Products ---\n",
        "print(\"\\nPopulating 'products' table...\")\n",
        "categories = ['Electronics', 'Books', 'Clothing', 'Home & Kitchen', 'Sports & Outdoors', 'Beauty', 'Toys']\n",
        "brands = [fake.company().split(' ')[0].replace(',', '') for _ in range(15)]\n",
        "product_data = []\n",
        "for i in range(1, NUM_PRODUCTS + 1):\n",
        "    cat = random.choice(categories)\n",
        "    # Generate a plausible product name\n",
        "    prod_name = f\"{random.choice(['Premium', 'Basic', 'Advanced', 'Eco', 'Pro'])} {cat.replace(' & ', '_').split(' ')[0]} {fake.word().capitalize()} {random.choice(['Widget', 'Device', 'Gear', 'Essentials', 'Kit'])}\"\n",
        "    product_data.append((i, prod_name, cat, random.choice(brands), round(random.uniform(5.0, 1000.0), 2)))\n",
        "\n",
        "for p_data in product_data: run_sql(\"INSERT INTO products (product_id, product_name, category, brand, price) VALUES (%s, %s, %s, %s, %s);\", p_data, fetch_results=False, quiet=True)\n",
        "print(f\"✅ {len(product_data)} products inserted.\")\n",
        "\n",
        "# --- Populate Interactions ---\n",
        "# (Interaction population logic remains the same)\n",
        "print(\"\\nPopulating 'interactions' table...\")\n",
        "interaction_types = ['view'] * 7 + ['add_to_cart'] * 2 + ['purchase'] * 1 # Skew towards views\n",
        "interaction_data = []\n",
        "for _ in range(NUM_INTERACTIONS):\n",
        "    user_id = random.randint(1, NUM_USERS)\n",
        "    product_id = random.randint(1, NUM_PRODUCTS)\n",
        "    itype = random.choice(interaction_types)\n",
        "    rating = None\n",
        "    if itype == 'purchase': rating = round(random.uniform(3.5, 5.0), 1)\n",
        "    elif itype == 'add_to_cart': rating = round(random.uniform(2.5, 4.5), 1)\n",
        "    elif random.random() < 0.1: rating = round(random.uniform(1.0, 5.0), 1)\n",
        "    ts = datetime.now() - timedelta(days=random.randint(0, 180), hours=random.randint(0,23), minutes=random.randint(0,59))\n",
        "    interaction_data.append((user_id, product_id, itype, rating, ts))\n",
        "\n",
        "for i_data in interaction_data: run_sql(\"INSERT INTO interactions (user_id, product_id, interaction_type, rating, timestamp) VALUES (%s, %s, %s, %s, %s);\", i_data, fetch_results=False, quiet=True)\n",
        "print(f\"✅ {len(interaction_data)} interactions inserted.\")\n",
        "\n",
        "\n",
        "# --- Sample Data Verification ---\n",
        "print(\"\\n--- Sample Data Verification ---\")\n",
        "print(\"Sample Users (first 3):\"); print(run_sql(\"SELECT * FROM users LIMIT 3;\").to_string())\n",
        "print(\"\\nSample Products (first 3):\"); print(run_sql(\"SELECT product_id, product_name, category FROM products LIMIT 3;\").to_string()) # Added product_name\n",
        "print(\"\\nSample Interactions (first 3, with product name for context):\")\n",
        "sample_interactions_sql = \"\"\"\n",
        "SELECT i.user_id, i.product_id, p.product_name, i.interaction_type, i.rating\n",
        "FROM interactions i JOIN products p ON i.product_id = p.product_id\n",
        "ORDER BY i.timestamp DESC LIMIT 3;\n",
        "\"\"\"\n",
        "print(run_sql(sample_interactions_sql).to_string())\n",
        "print(\"\\n--- End of Cell 4 ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkcmM_ZRPePN",
        "outputId": "544a1cfe-98c4-41ac-df32-89b7e7c8b5b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Generating Synthetic Data & Setting Up Schema ---\n",
            "Parameters: 150 users, 70 products, 750 interactions.\n",
            "\n",
            "Dropping existing tables (if any)...\n",
            "✅ Dropped existing project-related tables and monitoring schema.\n",
            "\n",
            "Creating database tables...\n",
            " Kicking off run_sql\n",
            "  Connecting to DB...\n",
            "  Connection successful. Executing query...\n",
            "  Query executed.\n",
            "  Committed (if applicable).\n",
            "  Database connection closed.\n",
            " Kicking off run_sql\n",
            "  Connecting to DB...\n",
            "  Connection successful. Executing query...\n",
            "  Query executed.\n",
            "  Committed (if applicable).\n",
            "  Database connection closed.\n",
            " Kicking off run_sql\n",
            "  Connecting to DB...\n",
            "  Connection successful. Executing query...\n",
            "  Query executed.\n",
            "  Committed (if applicable).\n",
            "  Database connection closed.\n",
            "✅ Core tables created: users, products, interactions.\n",
            "\n",
            "Populating 'users' table...\n",
            "✅ 150 users inserted.\n",
            "\n",
            "Populating 'products' table...\n",
            "✅ 70 products inserted.\n",
            "\n",
            "Populating 'interactions' table...\n",
            "✅ 750 interactions inserted.\n",
            "\n",
            "--- Sample Data Verification ---\n",
            "Sample Users (first 3):\n",
            " Kicking off run_sql\n",
            "  Connecting to DB...\n",
            "  Connection successful. Executing query...\n",
            "  Query executed.\n",
            "  Fetched 3 rows.\n",
            "  Database connection closed.\n",
            "   user_id  age  gender country_code\n",
            "0        1   42  Female           MV\n",
            "1        2   20  Female           PS\n",
            "2        3   50  Female           NL\n",
            "\n",
            "Sample Products (first 3):\n",
            " Kicking off run_sql\n",
            "  Connecting to DB...\n",
            "  Connection successful. Executing query...\n",
            "  Query executed.\n",
            "  Fetched 3 rows.\n",
            "  Database connection closed.\n",
            "   product_id                         product_name           category\n",
            "0           1                 Eco Toys Save Widget               Toys\n",
            "1           2  Eco Sports_Outdoors Remember Widget  Sports & Outdoors\n",
            "2           3   Basic Clothing Democrat Essentials           Clothing\n",
            "\n",
            "Sample Interactions (first 3, with product name for context):\n",
            " Kicking off run_sql\n",
            "  Connecting to DB...\n",
            "  Connection successful. Executing query...\n",
            "  Query executed.\n",
            "  Fetched 3 rows.\n",
            "  Database connection closed.\n",
            "   user_id  product_id                     product_name interaction_type  rating\n",
            "0       42          58          Advanced Toys Your Gear             view     NaN\n",
            "1       40          67  Basic Clothing Smile Essentials             view     NaN\n",
            "2       52          11        Basic Beauty Guess Widget      add_to_cart     2.9\n",
            "\n",
            "--- End of Cell 4 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5: FEATURE ENGINEERING & TRAINING DATA PREPARATION (WITH SCALING & PRINT FIX)\n",
        "#@markdown Creates feature tables, scales numerical features (age, price), and the final `training_data` table.\n",
        "print(\"--- Feature Engineering & Training Data Preparation (with Scaling) ---\")\n",
        "\n",
        "# --- Get Min/Max for scaling from original tables ---\n",
        "print(\"\\nFetching min/max for Age and Price for scaling...\")\n",
        "age_stats_df = run_sql(\"SELECT MIN(age) as min_age, MAX(age) as max_age FROM users;\", quiet=True)\n",
        "price_stats_df = run_sql(\"SELECT MIN(price) as min_price, MAX(price) as max_price FROM products;\", quiet=True)\n",
        "\n",
        "MIN_AGE = age_stats_df.iloc[0]['min_age'] if not age_stats_df.empty and age_stats_df.iloc[0]['min_age'] is not None else 18.0\n",
        "MAX_AGE = age_stats_df.iloc[0]['max_age'] if not age_stats_df.empty and age_stats_df.iloc[0]['max_age'] is not None else 70.0\n",
        "MIN_PRICE = price_stats_df.iloc[0]['min_price'] if not price_stats_df.empty and price_stats_df.iloc[0]['min_price'] is not None else 0.0\n",
        "MAX_PRICE = price_stats_df.iloc[0]['max_price'] if not price_stats_df.empty and price_stats_df.iloc[0]['max_price'] is not None else 1000.0\n",
        "\n",
        "if MAX_AGE == MIN_AGE: MAX_AGE = MIN_AGE + 1.0\n",
        "if MAX_PRICE == MIN_PRICE: MAX_PRICE = MIN_PRICE + 1.0\n",
        "DEFAULT_AGE_SCALED = ( (MIN_AGE + (MAX_AGE-MIN_AGE)/2) - MIN_AGE) / (MAX_AGE - MIN_AGE) if (MAX_AGE - MIN_AGE) != 0 else 0.5\n",
        "DEFAULT_PRICE_SCALED = ( (MIN_PRICE + (MAX_PRICE-MIN_PRICE)/2) - MIN_PRICE) / (MAX_PRICE - MIN_PRICE) if (MAX_PRICE - MIN_PRICE) != 0 else 0.5\n",
        "\n",
        "print(f\"Scaling params: Age (min:{MIN_AGE}, max:{MAX_AGE}), Price (min:{MIN_PRICE}, max:{MAX_PRICE})\")\n",
        "\n",
        "print(\"\\nDropping existing feature and training data tables...\")\n",
        "run_sql(\"DROP TABLE IF EXISTS training_data CASCADE;\", fetch_results=False, quiet=True)\n",
        "run_sql(\"DROP TABLE IF EXISTS user_features CASCADE;\", fetch_results=False, quiet=True)\n",
        "run_sql(\"DROP TABLE IF EXISTS product_features CASCADE;\", fetch_results=False, quiet=True)\n",
        "print(\"✅ Dropped tables.\")\n",
        "\n",
        "print(\"\\nCreating and populating 'user_features' table (with Age scaling)...\")\n",
        "run_sql(\"\"\"CREATE TABLE user_features (user_id INTEGER PRIMARY KEY REFERENCES users(user_id) ON DELETE CASCADE, features FLOAT[]);\"\"\", fetch_results=False)\n",
        "populate_user_features_sql = f\"\"\"\n",
        "INSERT INTO user_features (user_id, features)\n",
        "SELECT user_id, ARRAY[\n",
        "        CASE WHEN {MAX_AGE} - {MIN_AGE} = 0 THEN {DEFAULT_AGE_SCALED} ELSE (COALESCE(age::FLOAT, {MIN_AGE + (MAX_AGE-MIN_AGE)/2}) - {MIN_AGE}) / ({MAX_AGE} - {MIN_AGE}) END,\n",
        "        (CASE WHEN gender = 'Male' THEN 1.0 ELSE 0.0 END), (CASE WHEN gender = 'Female' THEN 1.0 ELSE 0.0 END), (CASE WHEN gender = 'Other' THEN 1.0 ELSE 0.0 END)\n",
        "    ] FROM users;\"\"\"\n",
        "run_sql(populate_user_features_sql, fetch_results=False, quiet=True)\n",
        "print(f\"✅ 'user_features' populated. Sample:\")\n",
        "user_features_sample_df = run_sql(\"SELECT user_id, features FROM user_features LIMIT 3;\")\n",
        "print(user_features_sample_df.to_string())\n",
        "USER_FEATURE_LENGTH = user_features_sample_df.iloc[0]['features'].__len__() if not user_features_sample_df.empty and user_features_sample_df.iloc[0]['features'] is not None else 0\n",
        "if USER_FEATURE_LENGTH > 0: print(f\"==> Detected User Feature Length: {USER_FEATURE_LENGTH}\")\n",
        "\n",
        "print(\"\\nCreating and populating 'product_features' table (with Price scaling)...\")\n",
        "run_sql(\"\"\"CREATE TABLE product_features (product_id INTEGER PRIMARY KEY REFERENCES products(product_id) ON DELETE CASCADE, features FLOAT[]);\"\"\", fetch_results=False)\n",
        "categories_df = run_sql(\"SELECT DISTINCT category FROM products ORDER BY category;\", quiet=True)\n",
        "categories_list = categories_df['category'].tolist() if not categories_df.empty else []\n",
        "print(f\"Product categories for one-hot encoding: {categories_list}\")\n",
        "product_features_parts_sql_list = [f\"CASE WHEN {MAX_PRICE} - {MIN_PRICE} = 0 THEN {DEFAULT_PRICE_SCALED} ELSE (COALESCE(price::FLOAT, {MIN_PRICE + (MAX_PRICE-MIN_PRICE)/2}) - {MIN_PRICE}) / ({MAX_PRICE} - {MIN_PRICE}) END\"]\n",
        "for cat_name in categories_list:\n",
        "    escaped_cat_name = cat_name.replace(\"'\", \"''\")\n",
        "    product_features_parts_sql_list.append(f\"(CASE WHEN category = '{escaped_cat_name}' THEN 1.0 ELSE 0.0 END)\")\n",
        "product_features_array_sql_expr = \"ARRAY[\" + \", \".join(product_features_parts_sql_list) + \"]\"\n",
        "populate_product_features_sql = f\"INSERT INTO product_features (product_id, features) SELECT product_id, {product_features_array_sql_expr} FROM products;\"\n",
        "run_sql(populate_product_features_sql, fetch_results=False, quiet=True)\n",
        "print(f\"✅ 'product_features' populated. Sample:\")\n",
        "product_features_sample_df = run_sql(\"SELECT product_id, features FROM product_features LIMIT 3;\")\n",
        "print(product_features_sample_df.to_string())\n",
        "PRODUCT_FEATURE_LENGTH = product_features_sample_df.iloc[0]['features'].__len__() if not product_features_sample_df.empty and product_features_sample_df.iloc[0]['features'] is not None else 0\n",
        "if PRODUCT_FEATURE_LENGTH > 0: print(f\"==> Detected Product Feature Length: {PRODUCT_FEATURE_LENGTH}\")\n",
        "\n",
        "print(\"\\nCreating and populating 'training_data' table...\")\n",
        "run_sql(\"\"\"CREATE TABLE training_data (interaction_id INTEGER PRIMARY KEY REFERENCES interactions(interaction_id) ON DELETE CASCADE, features FLOAT[], target FLOAT);\"\"\", fetch_results=False)\n",
        "populate_training_data_sql = \"\"\"INSERT INTO training_data (interaction_id, features, target)\n",
        "SELECT i.interaction_id, uf.features || pf.features, i.rating FROM interactions i\n",
        "JOIN user_features uf ON i.user_id = uf.user_id JOIN product_features pf ON i.product_id = pf.product_id\n",
        "WHERE i.rating IS NOT NULL AND uf.features IS NOT NULL AND pf.features IS NOT NULL;\"\"\"\n",
        "run_sql(populate_training_data_sql, fetch_results=False, quiet=True)\n",
        "training_data_count_df = run_sql(\"SELECT COUNT(*) as count, AVG(array_length(features, 1)) as avg_num_features FROM training_data;\", quiet=True)\n",
        "TRAINING_FEATURE_LENGTH = None\n",
        "avg_feat_len_val = None # Renamed to avoid conflict\n",
        "if not training_data_count_df.empty:\n",
        "    print(f\"✅ 'training_data' populated with {training_data_count_df.iloc[0]['count']} samples.\")\n",
        "    avg_feat_len_val = training_data_count_df.iloc[0]['avg_num_features']\n",
        "    if avg_feat_len_val is not None:\n",
        "        TRAINING_FEATURE_LENGTH = int(avg_feat_len_val)\n",
        "        print(f\"   Average (and expected) number of features per sample: {TRAINING_FEATURE_LENGTH}\")\n",
        "    else: print(\"   Could not determine average number of features.\")\n",
        "else: print(\"🔴 Failed to populate training_data or count it.\")\n",
        "\n",
        "print(\"Sample training data (first_feat, last_feat if possible, target):\") # Corrected print statement logic below\n",
        "if TRAINING_FEATURE_LENGTH is not None and TRAINING_FEATURE_LENGTH > 0:\n",
        "    last_feature_index = TRAINING_FEATURE_LENGTH\n",
        "    sample_training_data_sql = f\"SELECT interaction_id, features[1] as first_feat, features[{last_feature_index}] as last_feat, target FROM training_data WHERE features IS NOT NULL AND array_length(features,1) = {TRAINING_FEATURE_LENGTH} LIMIT 3;\"\n",
        "    print(run_sql(sample_training_data_sql).to_string())\n",
        "else:\n",
        "    sample_training_data_sql_fallback = \"SELECT interaction_id, features[1] as first_feat, target FROM training_data WHERE features IS NOT NULL AND array_length(features,1) >= 1 LIMIT 3;\"\n",
        "    print(run_sql(sample_training_data_sql_fallback).to_string())\n",
        "    print(\" (Could not determine last feature index as TRAINING_FEATURE_LENGTH was unknown or zero)\")\n",
        "\n",
        "if USER_FEATURE_LENGTH > 0 and PRODUCT_FEATURE_LENGTH > 0 and TRAINING_FEATURE_LENGTH is not None:\n",
        "    if USER_FEATURE_LENGTH + PRODUCT_FEATURE_LENGTH == TRAINING_FEATURE_LENGTH:\n",
        "        print(f\"✅ Sanity Check: User features ({USER_FEATURE_LENGTH}) + Product features ({PRODUCT_FEATURE_LENGTH}) = Training features ({TRAINING_FEATURE_LENGTH}). Lengths match!\")\n",
        "    else:\n",
        "        print(f\"🔴 WARNING Sanity Check: User features ({USER_FEATURE_LENGTH}) + Product features ({PRODUCT_FEATURE_LENGTH}) = {USER_FEATURE_LENGTH + PRODUCT_FEATURE_LENGTH} != Training features ({TRAINING_FEATURE_LENGTH}). Length mismatch!\")\n",
        "\n",
        "print(\"\\n--- Detailed Inspection of 'training_data' for problematic values ---\")\n",
        "nan_inf_check_sql = \"SELECT SUM(CASE WHEN EXISTS (SELECT 1 FROM unnest(features) f WHERE f = 'Infinity'::float OR f = '-Infinity'::float OR f = 'NaN'::float) THEN 1 ELSE 0 END) as rows_with_nan_or_inf_in_features FROM training_data;\"\n",
        "nan_inf_df = run_sql(nan_inf_check_sql)\n",
        "if not nan_inf_df.empty and nan_inf_df.iloc[0]['rows_with_nan_or_inf_in_features'] is not None:\n",
        "    print(\"NaN/Infinity Check in 'features' array elements:\"); print(nan_inf_df.to_string())\n",
        "    if nan_inf_df.iloc[0]['rows_with_nan_or_inf_in_features'] > 0: print(\"🔴 CRITICAL: Found rows with NaN or Infinity in features array!\")\n",
        "else: print(\"Could not perform NaN/Infinity check on features.\")\n",
        "nan_inf_target_check_sql = \"SELECT SUM(CASE WHEN target = 'Infinity'::float OR target = '-Infinity'::float OR target = 'NaN'::float THEN 1 ELSE 0 END) as nan_or_inf_in_target FROM training_data;\"\n",
        "nan_inf_target_df = run_sql(nan_inf_target_check_sql)\n",
        "if not nan_inf_target_df.empty and nan_inf_target_df.iloc[0]['nan_or_inf_in_target'] is not None:\n",
        "    print(\"\\nNaN/Infinity Check in 'target' column:\"); print(nan_inf_target_df.to_string())\n",
        "    if nan_inf_target_df.iloc[0]['nan_or_inf_in_target'] > 0: print(\"🔴 CRITICAL: Found NaN or Infinity in target column!\")\n",
        "else: print(\"Could not perform NaN/Infinity check on target.\")\n",
        "\n",
        "print(\"\\n--- Verifying scaled feature ranges (Age and Price elements should be ~0-1) ---\")\n",
        "scaled_features_sample_df = run_sql(\"SELECT features FROM training_data LIMIT 5;\")\n",
        "if not scaled_features_sample_df.empty:\n",
        "    print(\"Sample of first few feature arrays in training_data (after scaling):\")\n",
        "    for idx, row_val in scaled_features_sample_df.iterrows(): # Changed row to row_val\n",
        "        print(f\"  Row {idx}: {row_val['features']}\")\n",
        "else: print(\"Could not fetch scaled features for verification.\")\n",
        "print(\"\\n--- End of Cell 5 ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ky9q09NcPiae",
        "outputId": "07f86d11-34f3-4c44-adb2-633fb2ddc8e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Feature Engineering & Training Data Preparation (with Scaling) ---\n",
            "\n",
            "Fetching min/max for Age and Price for scaling...\n",
            "Scaling params: Age (min:18, max:70), Price (min:12.81, max:999.42)\n",
            "\n",
            "Dropping existing feature and training data tables...\n",
            "✅ Dropped tables.\n",
            "\n",
            "Creating and populating 'user_features' table (with Age scaling)...\n",
            " Kicking off run_sql\n",
            "  Connecting to DB...\n",
            "  Connection successful. Executing query...\n",
            "  Query executed.\n",
            "  Committed (if applicable).\n",
            "  Database connection closed.\n",
            "✅ 'user_features' populated. Sample:\n",
            " Kicking off run_sql\n",
            "  Connecting to DB...\n",
            "  Connection successful. Executing query...\n",
            "  Query executed.\n",
            "  Fetched 3 rows.\n",
            "  Database connection closed.\n",
            "   user_id                               features\n",
            "0        1   [0.46153846153846156, 0.0, 1.0, 0.0]\n",
            "1        2  [0.038461538461538464, 0.0, 1.0, 0.0]\n",
            "2        3    [0.6153846153846154, 0.0, 1.0, 0.0]\n",
            "==> Detected User Feature Length: 4\n",
            "\n",
            "Creating and populating 'product_features' table (with Price scaling)...\n",
            " Kicking off run_sql\n",
            "  Connecting to DB...\n",
            "  Connection successful. Executing query...\n",
            "  Query executed.\n",
            "  Committed (if applicable).\n",
            "  Database connection closed.\n",
            "Product categories for one-hot encoding: ['Beauty', 'Books', 'Clothing', 'Electronics', 'Home & Kitchen', 'Sports & Outdoors', 'Toys']\n",
            "✅ 'product_features' populated. Sample:\n",
            " Kicking off run_sql\n",
            "  Connecting to DB...\n",
            "  Connection successful. Executing query...\n",
            "  Query executed.\n",
            "  Fetched 3 rows.\n",
            "  Database connection closed.\n",
            "   product_id                                                  features\n",
            "0           1    [0.871185169418514, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n",
            "1           2  [0.44122804350249845, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
            "2           3   [0.8845541804765814, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
            "==> Detected Product Feature Length: 8\n",
            "\n",
            "Creating and populating 'training_data' table...\n",
            " Kicking off run_sql\n",
            "  Connecting to DB...\n",
            "  Connection successful. Executing query...\n",
            "  Query executed.\n",
            "  Committed (if applicable).\n",
            "  Database connection closed.\n",
            "✅ 'training_data' populated with 274 samples.\n",
            "   Average (and expected) number of features per sample: 12\n",
            "Sample training data (first_feat, last_feat if possible, target):\n",
            " Kicking off run_sql\n",
            "  Connecting to DB...\n",
            "  Connection successful. Executing query...\n",
            "  Query executed.\n",
            "  Fetched 3 rows.\n",
            "  Database connection closed.\n",
            "   interaction_id  first_feat  last_feat  target\n",
            "0               1    0.384615        0.0     2.6\n",
            "1               3    0.615385        0.0     4.4\n",
            "2              13    0.230769        0.0     3.8\n",
            "✅ Sanity Check: User features (4) + Product features (8) = Training features (12). Lengths match!\n",
            "\n",
            "--- Detailed Inspection of 'training_data' for problematic values ---\n",
            " Kicking off run_sql\n",
            "  Connecting to DB...\n",
            "  Connection successful. Executing query...\n",
            "  Query executed.\n",
            "  Fetched 1 rows.\n",
            "  Database connection closed.\n",
            "NaN/Infinity Check in 'features' array elements:\n",
            "   rows_with_nan_or_inf_in_features\n",
            "0                                 0\n",
            " Kicking off run_sql\n",
            "  Connecting to DB...\n",
            "  Connection successful. Executing query...\n",
            "  Query executed.\n",
            "  Fetched 1 rows.\n",
            "  Database connection closed.\n",
            "\n",
            "NaN/Infinity Check in 'target' column:\n",
            "   nan_or_inf_in_target\n",
            "0                     0\n",
            "\n",
            "--- Verifying scaled feature ranges (Age and Price elements should be ~0-1) ---\n",
            " Kicking off run_sql\n",
            "  Connecting to DB...\n",
            "  Connection successful. Executing query...\n",
            "  Query executed.\n",
            "  Fetched 5 rows.\n",
            "  Database connection closed.\n",
            "Sample of first few feature arrays in training_data (after scaling):\n",
            "  Row 0: [0.38461538461538464, 0.0, 0.0, 1.0, 0.7751087055675495, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
            "  Row 1: [0.6153846153846154, 1.0, 0.0, 0.0, 0.7751087055675495, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
            "  Row 2: [0.23076923076923078, 1.0, 0.0, 0.0, 0.2507475091474848, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "  Row 3: [0.8653846153846154, 0.0, 0.0, 1.0, 0.7117706084471067, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]\n",
            "  Row 4: [0.9230769230769231, 1.0, 0.0, 0.0, 0.14174800579763025, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "\n",
            "--- End of Cell 5 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6: MODEL TRAINING WITH `pgml.train`\n",
        "#@markdown Trains a regression model to predict product ratings using the prepared `training_data`.\n",
        "\n",
        "PROJECT_NAME = 'product_recommender_v1' #@param {type:\"string\"}\n",
        "CHOSEN_ALGORITHM = 'linear' #@param [\"linear\", \"xgboost\", \"lightgbm\"]\n",
        "EXPLICIT_AUTO_DEPLOY = True #@param {type:\"boolean\"}\n",
        "ADD_MINIMAL_HYPERPARAMS = False #@param {type:\"boolean\"}\n",
        "\n",
        "print(f\"--- Training Recommendation Model (Project: '{PROJECT_NAME}', Algorithm: '{CHOSEN_ALGORITHM}', AutoDeploy: {EXPLICIT_AUTO_DEPLOY}, MinimalHyperparams: {ADD_MINIMAL_HYPERPARAMS}) ---\")\n",
        "\n",
        "print(\"\\n--- Verifying 'training_data' content before training (condensed) ---\")\n",
        "training_data_check_sql = \"\"\"\n",
        "SELECT COUNT(*) as training_samples\n",
        "FROM training_data\n",
        "WHERE features IS NOT NULL AND target IS NOT NULL\n",
        "  AND NOT EXISTS (SELECT 1 FROM unnest(features) f WHERE f IS NULL OR f = 'NaN'::float OR f = 'Infinity'::float OR f = '-Infinity'::float)\n",
        "  AND target NOT IN ('NaN'::float, 'Infinity'::float, '-Infinity'::float);\"\"\"\n",
        "training_data_check_df = run_sql(training_data_check_sql, quiet=True)\n",
        "valid_training_samples = 0\n",
        "if not training_data_check_df.empty and training_data_check_df.iloc[0]['training_samples'] is not None:\n",
        "    valid_training_samples = training_data_check_df.iloc[0]['training_samples']\n",
        "\n",
        "if valid_training_samples == 0:\n",
        "    print(\"🔴 CRITICAL: No valid training samples found. Cannot train model.\")\n",
        "    raise ValueError(\"No valid training data available.\")\n",
        "else:\n",
        "    print(f\"Found {valid_training_samples} presumably clean training samples.\")\n",
        "    if 'TRAINING_FEATURE_LENGTH' in globals() and TRAINING_FEATURE_LENGTH is not None:\n",
        "        print(f\"   Expected feature length from Cell 5: {TRAINING_FEATURE_LENGTH}\")\n",
        "\n",
        "    print(f\"\\nStarting model training for project '{PROJECT_NAME}' using 'regression' task and algorithm '{CHOSEN_ALGORITHM}'...\")\n",
        "\n",
        "    hyperparams_sql_string = \"\"\n",
        "    if ADD_MINIMAL_HYPERPARAMS:\n",
        "        if CHOSEN_ALGORITHM == 'xgboost':\n",
        "            hyperparams_sql_string = \", hyperparams => '{{\\\"n_estimators\\\": 20, \\\"max_depth\\\": 4, \\\"learning_rate\\\": 0.1}}'::jsonb\"\n",
        "            print(\"   Using minimal hyperparameters for xgboost.\")\n",
        "        elif CHOSEN_ALGORITHM == 'lightgbm':\n",
        "             hyperparams_sql_string = \", hyperparams => '{{\\\"n_estimators\\\": 20, \\\"num_leaves\\\": 10, \\\"learning_rate\\\": 0.1}}'::jsonb\"\n",
        "             print(\"   Using minimal hyperparameters for lightgbm.\")\n",
        "        elif CHOSEN_ALGORITHM == 'linear':\n",
        "            hyperparams_sql_string = \", hyperparams => '{{}}'::jsonb\"\n",
        "            print(\"   Using empty hyperparameters for linear.\")\n",
        "\n",
        "    train_statement_sql = f\"\"\"\n",
        "    SELECT pgml.train(\n",
        "        project_name => '{PROJECT_NAME}',\n",
        "        task => 'regression',\n",
        "        relation_name => 'training_data',\n",
        "        y_column_name => 'target',\n",
        "        algorithm => '{CHOSEN_ALGORITHM}',\n",
        "        automatic_deploy => {str(EXPLICIT_AUTO_DEPLOY).lower()}\n",
        "        {hyperparams_sql_string}\n",
        "    );\"\"\"\n",
        "    print(f\"\\nExecuting training SQL:\\n{train_statement_sql}\")\n",
        "    training_start_time = time.time()\n",
        "    run_sql(train_statement_sql, fetch_results=False, quiet=False)\n",
        "    training_duration = time.time() - training_start_time\n",
        "    print(f\"\\n--- pgml.train Statement Executed (Duration: {training_duration:.2f} seconds) ---\")\n",
        "    print(\"If no SQL errors were printed by run_sql, pgml.train command was sent. Now checking pgml metadata & last_error_message...\")\n",
        "\n",
        "    print(\"\\nChecking for any specific pgml error messages (if pgml.last_error_message function exists)...\")\n",
        "    last_error_df = run_sql(\"SELECT pgml.last_error_message();\", quiet=True)\n",
        "    if not last_error_df.empty and last_error_df.iloc[0,0] is not None and str(last_error_df.iloc[0,0]).strip() != '':\n",
        "        print(f\"⚠️ pgml.last_error_message() reported: {last_error_df.iloc[0,0]}\")\n",
        "    else:\n",
        "        print(\"No specific error message from pgml.last_error_message().\")\n",
        "\n",
        "# --- Verification: Check pgml.projects and pgml.models tables ---\n",
        "print(f\"\\n--- Verifying Project '{PROJECT_NAME}' and its Models post-training attempt ---\")\n",
        "project_verify_df = run_sql(f\"SELECT id, name, task, algorithm_name as deployed_algorithm FROM pgml.projects WHERE name = '{PROJECT_NAME}';\", quiet=True)\n",
        "project_successfully_created = False # Flag to track actual project creation\n",
        "\n",
        "if not project_verify_df.empty:\n",
        "    print(f\"✅ Project '{PROJECT_NAME}' found in pgml.projects:\")\n",
        "    print(project_verify_df.to_string())\n",
        "    project_id = project_verify_df.iloc[0]['id']\n",
        "    project_successfully_created = True # Set flag\n",
        "    models_for_project_df = run_sql(f\"SELECT id as model_id, algorithm_name, status, metrics, to_char(created_at, 'YYYY-MM-DD HH24:MI:SS') as created_at FROM pgml.models WHERE project_id = {project_id} ORDER BY created_at DESC;\", quiet=True)\n",
        "    if not models_for_project_df.empty:\n",
        "        print(f\"\\nModels found in pgml.models for project_id {project_id} ('{PROJECT_NAME}'):\")\n",
        "        print(models_for_project_df.to_string())\n",
        "    else:\n",
        "        print(f\"⚠️ No models found in pgml.models for project_id {project_id} ('{PROJECT_NAME}'). Training might have failed to produce a model artifact.\")\n",
        "        project_successfully_created = False # Re-evaluate if no models present\n",
        "else:\n",
        "    print(f\"🔴 ERROR: Project '{PROJECT_NAME}' NOT found in pgml.projects after training attempt.\")\n",
        "    print(\"   Review SQL errors from `pgml.train` execution OR `pgml.last_error_message()` output.\")\n",
        "    project_successfully_created = False\n",
        "\n",
        "\n",
        "# --- Capstone Note on Model Training & Setting MOCK_MODEL_PREDICTIONS ---\n",
        "print(\"\\n--- Capstone Note on Model Training & Mocking Status ---\")\n",
        "if project_successfully_created:\n",
        "    print(f\"✅ Project '{PROJECT_NAME}' was successfully created/found. Real predictions will be attempted.\")\n",
        "    MOCK_MODEL_PREDICTIONS = False\n",
        "else:\n",
        "    print(f\"🔴 NOTE: Project '{PROJECT_NAME}' was NOT successfully created/found in pgml.projects.\")\n",
        "    print(\"   For this capstone, recommenders will use MOCKED (random) scores.\")\n",
        "    MOCK_MODEL_PREDICTIONS = True\n",
        "print(f\"Python variable MOCK_MODEL_PREDICTIONS set to: {MOCK_MODEL_PREDICTIONS}\")\n",
        "\n",
        "print(\"\\n--- End of Cell 6 ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMNTT2dGKo6v",
        "outputId": "a0a4c638-4464-4715-e20f-363cc3c99d7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training Recommendation Model (Project: 'product_recommender_v1', Algorithm: 'linear', AutoDeploy: True, MinimalHyperparams: False) ---\n",
            "\n",
            "--- Verifying 'training_data' content before training (condensed) ---\n",
            "Found 274 presumably clean training samples.\n",
            "   Expected feature length from Cell 5: 12\n",
            "\n",
            "Starting model training for project 'product_recommender_v1' using 'regression' task and algorithm 'linear'...\n",
            "\n",
            "Executing training SQL:\n",
            "\n",
            "    SELECT pgml.train(\n",
            "        project_name => 'product_recommender_v1',\n",
            "        task => 'regression',\n",
            "        relation_name => 'training_data',\n",
            "        y_column_name => 'target',\n",
            "        algorithm => 'linear',\n",
            "        automatic_deploy => true\n",
            "         \n",
            "    );\n",
            " Kicking off run_sql\n",
            "  Connecting to DB...\n",
            "  Connection successful. Executing query...\n",
            "  Query executed.\n",
            "  Committed (if applicable).\n",
            "  Database connection closed.\n",
            "\n",
            "--- pgml.train Statement Executed (Duration: 0.70 seconds) ---\n",
            "If no SQL errors were printed by run_sql, pgml.train command was sent. Now checking pgml metadata & last_error_message...\n",
            "\n",
            "Checking for any specific pgml error messages (if pgml.last_error_message function exists)...\n",
            "No specific error message from pgml.last_error_message().\n",
            "\n",
            "--- Verifying Project 'product_recommender_v1' and its Models post-training attempt ---\n",
            "🔴 ERROR: Project 'product_recommender_v1' NOT found in pgml.projects after training attempt.\n",
            "   Review SQL errors from `pgml.train` execution OR `pgml.last_error_message()` output.\n",
            "\n",
            "--- Capstone Note on Model Training & Mocking Status ---\n",
            "🔴 NOTE: Project 'product_recommender_v1' was NOT successfully created/found in pgml.projects.\n",
            "   For this capstone, recommenders will use MOCKED (random) scores.\n",
            "Python variable MOCK_MODEL_PREDICTIONS set to: True\n",
            "\n",
            "--- End of Cell 6 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7 (Using a custom TYPE for temp_recommendations array)\n",
        "\n",
        "print(f\"--- Creating SQL Function for Real-time Recommendations ('get_recommendations') ---\")\n",
        "\n",
        "if 'PROJECT_NAME' not in globals():\n",
        "    PROJECT_NAME = 'product_recommender_v1'\n",
        "    print(f\"⚠️ PROJECT_NAME not found globally, using default: '{PROJECT_NAME}'\")\n",
        "print(f\"This function will use the trained model: '{PROJECT_NAME}'\")\n",
        "\n",
        "if 'TRAINING_FEATURE_LENGTH' not in globals() or TRAINING_FEATURE_LENGTH is None:\n",
        "    print(\"⚠️ TRAINING_FEATURE_LENGTH not found globally from Cell 5, attempting to re-fetch...\")\n",
        "    refetch_length_df = run_sql(\"SELECT array_length(features, 1) as len FROM training_data WHERE features IS NOT NULL LIMIT 1;\", quiet=True)\n",
        "    if not refetch_length_df.empty and refetch_length_df.iloc[0]['len'] is not None:\n",
        "        TRAINING_FEATURE_LENGTH = int(refetch_length_df.iloc[0]['len'])\n",
        "        print(f\"   Re-fetched TRAINING_FEATURE_LENGTH: {TRAINING_FEATURE_LENGTH}\")\n",
        "    else:\n",
        "        TRAINING_FEATURE_LENGTH = 0\n",
        "        print(f\"   🔴 Could not re-fetch TRAINING_FEATURE_LENGTH. Set to 0.\")\n",
        "\n",
        "if 'MOCK_MODEL_PREDICTIONS' not in globals():\n",
        "    print(\"WARNING: MOCK_MODEL_PREDICTIONS not globally set from Cell 6. Checking project existence now.\")\n",
        "    project_exists_check_df = run_sql(f\"SELECT 1 FROM pgml.projects WHERE name = '{PROJECT_NAME}';\", quiet=True)\n",
        "    if project_exists_check_df.empty:\n",
        "        print(f\"   Project '{PROJECT_NAME}' not found. Enabling mock mode for get_recommendations.\")\n",
        "        MOCK_MODEL_PREDICTIONS = True\n",
        "    else:\n",
        "        print(f\"   Project '{PROJECT_NAME}' found. Disabling mock mode for get_recommendations.\")\n",
        "        MOCK_MODEL_PREDICTIONS = False\n",
        "else:\n",
        "    print(f\"MOCK_MODEL_PREDICTIONS is globally set to: {MOCK_MODEL_PREDICTIONS} (from Cell 6).\")\n",
        "\n",
        "# --- Create a TYPE to match the function's RETURNS TABLE structure ---\n",
        "type_definition_sql = \"\"\"\n",
        "DROP TYPE IF EXISTS recommendation_result_type CASCADE;\n",
        "CREATE TYPE recommendation_result_type AS (\n",
        "    product_id INTEGER,\n",
        "    product_name TEXT,\n",
        "    category TEXT,\n",
        "    predicted_score FLOAT\n",
        ");\n",
        "\"\"\"\n",
        "run_sql(type_definition_sql, fetch_results=False)\n",
        "print(\"✅ Custom type 'recommendation_result_type' created/recreated.\")\n",
        "\n",
        "\n",
        "print(\"\\nDropping existing 'get_recommendations' function signatures...\")\n",
        "run_sql(\"DROP FUNCTION IF EXISTS get_recommendations(INTEGER, INTEGER);\", fetch_results=False, quiet=True)\n",
        "run_sql(\"DROP FUNCTION IF EXISTS get_recommendations(INTEGER, INTEGER, BOOLEAN);\", fetch_results=False, quiet=True)\n",
        "print(\"✅ Attempted to drop existing function signatures.\")\n",
        "\n",
        "get_recommendations_sql = f\"\"\"\n",
        "CREATE OR REPLACE FUNCTION get_recommendations(\n",
        "    p_user_id INTEGER,\n",
        "    p_top_n INTEGER DEFAULT 5,\n",
        "    p_force_mock_predictions BOOLEAN DEFAULT {str(MOCK_MODEL_PREDICTIONS).lower()}\n",
        ")\n",
        "RETURNS SETOF recommendation_result_type -- Use the custom type here\n",
        "AS $$\n",
        "DECLARE\n",
        "    v_model_actually_exists BOOLEAN := false;\n",
        "    v_user_features FLOAT[];\n",
        "    v_combined_features FLOAT[];\n",
        "    v_prediction FLOAT;\n",
        "    r RECORD;\n",
        "    -- Declare temp_recommendations as an array of our custom type\n",
        "    temp_recommendations recommendation_result_type[];\n",
        "    recommendation_row recommendation_result_type; -- Use the custom type for the row variable\n",
        "BEGIN\n",
        "    RAISE NOTICE 'get_recommendations: Called for user_id=%, top_n=%, force_mock=%', p_user_id, p_top_n, p_force_mock_predictions;\n",
        "\n",
        "    SELECT features INTO v_user_features FROM user_features WHERE user_id = p_user_id LIMIT 1;\n",
        "    IF v_user_features IS NULL OR array_length(v_user_features, 1) = 0 THEN\n",
        "        RAISE WARNING 'get_recommendations: User % has NULL or empty features. Forcing mock.', p_user_id;\n",
        "        p_force_mock_predictions := true;\n",
        "    END IF;\n",
        "\n",
        "    IF NOT p_force_mock_predictions THEN\n",
        "        SELECT EXISTS (SELECT 1 FROM pgml.projects WHERE name = '{PROJECT_NAME}') INTO v_model_actually_exists;\n",
        "        RAISE NOTICE 'get_recommendations: Model \"{PROJECT_NAME}\" existence check: %', v_model_actually_exists;\n",
        "    END IF;\n",
        "\n",
        "    IF NOT p_force_mock_predictions AND v_model_actually_exists THEN\n",
        "        RAISE NOTICE 'get_recommendations: Attempting REAL predictions with model \"{PROJECT_NAME}\".';\n",
        "        FOR r IN (\n",
        "            SELECT p.product_id as current_pid, p.product_name as current_pname,\n",
        "                   p.category as current_pcat, pf.features as current_pf_features\n",
        "            FROM products p JOIN product_features pf ON p.product_id = pf.product_id\n",
        "            WHERE p.product_id NOT IN (\n",
        "                    SELECT i.product_id FROM interactions i\n",
        "                    WHERE i.user_id = p_user_id AND (i.interaction_type = 'purchase' OR i.rating >= 4.0)\n",
        "                )\n",
        "                AND v_user_features IS NOT NULL AND array_length(v_user_features, 1) > 0\n",
        "                AND pf.features IS NOT NULL AND array_length(pf.features, 1) > 0\n",
        "        ) LOOP\n",
        "            v_combined_features := NULL; v_prediction := NULL;\n",
        "\n",
        "            IF r.current_pf_features IS NOT NULL AND array_length(r.current_pf_features, 1) > 0 AND\n",
        "               v_user_features IS NOT NULL AND array_length(v_user_features, 1) > 0 THEN\n",
        "                v_combined_features := v_user_features || r.current_pf_features;\n",
        "                IF array_length(v_combined_features, 1) = ({TRAINING_FEATURE_LENGTH if TRAINING_FEATURE_LENGTH != -1 else 0}) THEN\n",
        "                    BEGIN\n",
        "                        RAISE NOTICE 'get_recommendations: Predicting P:% UF_len:% PF_len:% Combined_len:%',\n",
        "                                     r.current_pid, array_length(v_user_features,1), array_length(r.current_pf_features,1), array_length(v_combined_features,1);\n",
        "                        v_prediction := pgml.predict('{PROJECT_NAME}', v_combined_features);\n",
        "                    EXCEPTION\n",
        "                        WHEN others THEN\n",
        "                            RAISE WARNING 'get_recommendations: pgml.predict FAILED for P:% - SQLSTATE:%, SQLERRM:%', r.current_pid, SQLSTATE, SQLERRM;\n",
        "                            v_prediction := NULL;\n",
        "                    END;\n",
        "                ELSE\n",
        "                    RAISE WARNING 'get_recommendations: Bad feature vector length for P:% U:%. UF_len:% PF_len:% Combined_len:% Expected:%',\n",
        "                                 r.current_pid, p_user_id, array_length(v_user_features,1), array_length(r.current_pf_features,1),\n",
        "                                 array_length(v_combined_features,1), ({TRAINING_FEATURE_LENGTH if TRAINING_FEATURE_LENGTH != -1 else 0});\n",
        "                END IF;\n",
        "            END IF;\n",
        "\n",
        "            IF v_prediction IS NOT NULL THEN\n",
        "                 recommendation_row := (r.current_pid, r.current_pname, r.current_pcat, v_prediction::FLOAT)::recommendation_result_type;\n",
        "                 temp_recommendations := array_append(temp_recommendations, recommendation_row);\n",
        "            END IF;\n",
        "        END LOOP;\n",
        "\n",
        "        IF array_length(temp_recommendations, 1) > 0 THEN\n",
        "            -- Need to unnest and then order and limit.\n",
        "            -- Create a temporary table for sorting, then return from it.\n",
        "            DROP TABLE IF EXISTS __temp_recs_for_user;\n",
        "            CREATE TEMP TABLE __temp_recs_for_user AS\n",
        "            SELECT * FROM unnest(temp_recommendations);\n",
        "\n",
        "            RETURN QUERY SELECT tr.product_id, tr.product_name, tr.category, tr.predicted_score\n",
        "                         FROM __temp_recs_for_user tr\n",
        "                         ORDER BY tr.predicted_score DESC NULLS LAST LIMIT p_top_n;\n",
        "        ELSE\n",
        "             RAISE NOTICE 'get_recommendations: No valid predictions made in real mode.';\n",
        "        END IF;\n",
        "\n",
        "    ELSE\n",
        "        IF p_force_mock_predictions THEN\n",
        "            RAISE NOTICE 'get_recommendations: Mock mode: Generating random scores for user %.', p_user_id;\n",
        "        ELSE\n",
        "             RAISE NOTICE 'get_recommendations: Model \"{PROJECT_NAME}\" not found: Generating random scores for user %.', p_user_id;\n",
        "        END IF;\n",
        "\n",
        "        RETURN QUERY\n",
        "        SELECT p.product_id, p.product_name, p.category, (random() * 2.0 + 2.5)::FLOAT\n",
        "        FROM products p\n",
        "        WHERE p.product_id NOT IN (\n",
        "              SELECT i.product_id FROM interactions i\n",
        "              WHERE i.user_id = p_user_id AND (i.interaction_type = 'purchase' OR i.rating >= 4.0)\n",
        "          )\n",
        "        ORDER BY random() LIMIT p_top_n; -- Random order for mock if scores are all random\n",
        "    END IF;\n",
        "END;\n",
        "$$ LANGUAGE plpgsql;\n",
        "\"\"\"\n",
        "run_sql(type_definition_sql, fetch_results=False) # Ensure type is created before function\n",
        "run_sql(get_recommendations_sql, fetch_results=False)\n",
        "print(f\"✅ SQL function 'get_recommendations' created/recreated (using custom type for temp array).\")\n",
        "\n",
        "# --- Test the SQL Recommendation Function ---\n",
        "# (The test block remains the same)\n",
        "print(\"\\n--- Testing 'get_recommendations' SQL Function ---\")\n",
        "test_user_id = 1\n",
        "test_top_n = 5\n",
        "print(f\"Fetching top {test_top_n} recommendations for user_id: {test_user_id} (Python MOCK_MODEL_PREDICTIONS var is: {MOCK_MODEL_PREDICTIONS})\")\n",
        "\n",
        "if 'NUM_USERS' in globals() and (test_user_id > NUM_USERS or test_user_id <=0):\n",
        "    print(f\"⚠️ Test user ID {test_user_id} is out of range (1-{NUM_USERS}). Using User 1 if NUM_USERS > 0 else skipping test.\")\n",
        "    if NUM_USERS > 0: test_user_id = 1\n",
        "    else: test_user_id = None\n",
        "\n",
        "if test_user_id is not None:\n",
        "    recommendations_df = run_sql(f\"SELECT * FROM get_recommendations(p_user_id => {test_user_id}, p_top_n => {test_top_n});\")\n",
        "    if not recommendations_df.empty:\n",
        "        print(f\"\\nRecommendations for User ID {test_user_id}:\")\n",
        "        print(recommendations_df.to_string())\n",
        "    else:\n",
        "        print(f\"⚠️ No recommendations returned for user_id {test_user_id}.\")\n",
        "        print(f\"   Check RAISE NOTICE/WARNING messages. Current MOCK_MODEL_PREDICTIONS={MOCK_MODEL_PREDICTIONS}\")\n",
        "else:\n",
        "    print(\"Skipping get_recommendations test as test_user_id is invalid or NUM_USERS is 0.\")\n",
        "\n",
        "print(\"\\n--- End of Cell 7 ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFWtAynnQUIk",
        "outputId": "6de760ce-66b1-4242-859f-e9012352e050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Creating SQL Function for Real-time Recommendations ('get_recommendations') ---\n",
            "This function will use the trained model: 'product_recommender_v1'\n",
            "MOCK_MODEL_PREDICTIONS is globally set to: True (from Cell 6).\n",
            " Kicking off run_sql\n",
            "  Connecting to DB...\n",
            "  Connection successful. Executing query...\n",
            "  Query executed.\n",
            "  Committed (if applicable).\n",
            "  Database connection closed.\n",
            "✅ Custom type 'recommendation_result_type' created/recreated.\n",
            "\n",
            "Dropping existing 'get_recommendations' function signatures...\n",
            "✅ Attempted to drop existing function signatures.\n",
            " Kicking off run_sql\n",
            "  Connecting to DB...\n",
            "  Connection successful. Executing query...\n",
            "  Query executed.\n",
            "  Committed (if applicable).\n",
            "  Database connection closed.\n",
            " Kicking off run_sql\n",
            "  Connecting to DB...\n",
            "  Connection successful. Executing query...\n",
            "  Query executed.\n",
            "  Committed (if applicable).\n",
            "  Database connection closed.\n",
            "✅ SQL function 'get_recommendations' created/recreated (using custom type for temp array).\n",
            "\n",
            "--- Testing 'get_recommendations' SQL Function ---\n",
            "Fetching top 5 recommendations for user_id: 1 (Python MOCK_MODEL_PREDICTIONS var is: True)\n",
            " Kicking off run_sql\n",
            "  Connecting to DB...\n",
            "  Connection successful. Executing query...\n",
            "  Query executed.\n",
            "  Fetched 5 rows.\n",
            "  Database connection closed.\n",
            "\n",
            "Recommendations for User ID 1:\n",
            "   product_id                   product_name  category  predicted_score\n",
            "0          21   Premium Clothing Market Gear  Clothing         2.569660\n",
            "1          51            Eco Books Meet Gear     Books         2.617618\n",
            "2          13       Premium Toys Enough Gear      Toys         2.632937\n",
            "3          14       Advanced Toys Buy Widget      Toys         2.668718\n",
            "4          45  Advanced Clothing Answer Gear  Clothing         2.670123\n",
            "\n",
            "--- End of Cell 7 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 8: API DESIGN WITH FASTAPI\n",
        "#@markdown This cell presents the Python code for a FastAPI application that would serve recommendations.\n",
        "#@markdown This code is for design and understanding; **it will not be run as a live server in this Colab notebook.**\n",
        "#@markdown You would typically save this to a `main.py` file and run it with `uvicorn main:app --reload`.\n",
        "\n",
        "print(\"--- Designing the FastAPI Application for Recommendations ---\")\n",
        "\n",
        "#@markdown #### Connection Pooling (as per Part 3, Slide 2)\n",
        "#@markdown Using a connection pool is crucial for performance in a web application.\n",
        "connection_pool_code = \"\"\"\n",
        "# main.py (Illustrative - For local execution with Uvicorn)\n",
        "import os\n",
        "from fastapi import FastAPI, HTTPException\n",
        "import psycopg2\n",
        "from psycopg2.pool import SimpleConnectionPool\n",
        "\n",
        "# --- Configuration ---\n",
        "# In a real app, use environment variables for sensitive info\n",
        "DATABASE_URL = os.environ.get(\"DATABASE_URL\", \"your_postgres_connection_string_here_from_cell_1\")\n",
        "# It's critical that this DATABASE_URL matches your actual DB_CONNECTION_STRING from Cell 1\n",
        "\n",
        "# Initialize a connection pool\n",
        "try:\n",
        "    pool = SimpleConnectionPool(\n",
        "        minconn=2,  # Minimum number of connections in the pool\n",
        "        maxconn=10, # Maximum number of connections\n",
        "        dsn=DATABASE_URL\n",
        "    )\n",
        "    print(\"Database connection pool initialized.\")\n",
        "except Exception as e:\n",
        "    print(f\"FATAL: Could not initialize database connection pool: {e}\")\n",
        "    pool = None # Ensure pool is None if initialization fails\n",
        "\n",
        "app = FastAPI(title=\"PostgresML Recommender API\")\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "async def startup_event():\n",
        "    if pool is None:\n",
        "        # This will prevent the app from starting if pool is not available\n",
        "        raise RuntimeError(\"Database connection pool is not available. FastAPI app cannot start.\")\n",
        "    print(\"FastAPI application started successfully with DB pool.\")\n",
        "\n",
        "@app.on_event(\"shutdown\")\n",
        "def shutdown_event():\n",
        "    if pool:\n",
        "        pool.closeall()\n",
        "        print(\"Database connection pool closed.\")\n",
        "\n",
        "# --- API Endpoint (as per Part 5, Slide 2) ---\n",
        "@app.get(\"/recommendations/{user_id}\", tags=[\"Recommendations\"])\n",
        "async def get_user_recommendations_api(user_id: int, top_n: int = 5):\n",
        "    '''\n",
        "    Fetches top N product recommendations for a given user_id.\n",
        "    '''\n",
        "    if pool is None:\n",
        "        raise HTTPException(status_code=503, detail=\"Database service unavailable\")\n",
        "\n",
        "    conn = None\n",
        "    try:\n",
        "        conn = pool.getconn() # Get a connection from the pool\n",
        "        with conn.cursor() as cur:\n",
        "            # Call the SQL function created in Cell 7\n",
        "            cur.execute(\"SELECT product_id, product_name, category, predicted_score FROM get_recommendations(%s, %s);\", (user_id, top_n))\n",
        "            recs_data = cur.fetchall()\n",
        "\n",
        "            if not recs_data:\n",
        "                # Return 404 if user exists but no recommendations, or handle as appropriate\n",
        "                # Check if user exists to differentiate\n",
        "                cur.execute(\"SELECT 1 FROM users WHERE user_id = %s;\", (user_id,))\n",
        "                if not cur.fetchone():\n",
        "                    raise HTTPException(status_code=404, detail=f\"User ID {user_id} not found.\")\n",
        "                return {\"user_id\": user_id, \"recommendations\": []} # User exists, no recs\n",
        "\n",
        "            recommendations = [\n",
        "                {\"product_id\": r[0], \"product_name\": r[1], \"category\": r[2], \"predicted_score\": round(r[3], 4)}\n",
        "                for r in recs_data\n",
        "            ]\n",
        "            return {\"user_id\": user_id, \"top_n_requested\": top_n, \"recommendations\": recommendations}\n",
        "    except psycopg2.Error as db_err:\n",
        "        # Log the database error, return a generic server error\n",
        "        print(f\"Database error for user_id {user_id}: {db_err}\")\n",
        "        raise HTTPException(status_code=500, detail=\"Internal database error\")\n",
        "    except Exception as e:\n",
        "        # Log other errors\n",
        "        print(f\"An unexpected error occurred for user_id {user_id}: {e}\")\n",
        "        raise HTTPException(status_code=500, detail=\"An unexpected internal server error occurred\")\n",
        "    finally:\n",
        "        if conn:\n",
        "            pool.putconn(conn) # Return the connection to the pool\n",
        "\n",
        "# To run this (locally, not in Colab):\n",
        "# 1. Save this code as `main.py`.\n",
        "# 2. Set the DATABASE_URL environment variable: `export DATABASE_URL=\"your_actual_connection_string\"`\n",
        "# 3. Run Uvicorn: `uvicorn main:app --reload --port 8000`\n",
        "# 4. Access in browser or with curl: `curl http://localhost:8000/recommendations/1?top_n=3`\n",
        "\"\"\"\n",
        "\n",
        "print(\"--- FastAPI Application Code (for main.py) ---\")\n",
        "# Display the code using IPython.display.Code for better formatting in Colab\n",
        "from IPython.display import Code\n",
        "Code(connection_pool_code, language='python')\n",
        "\n",
        "print(\"\\nTo make this API live:\")\n",
        "print(\"1. Replace 'your_postgres_connection_string_here_from_cell_1' with your actual DB_CONNECTION_STRING from Cell 1 (or better, use an environment variable).\")\n",
        "print(\"2. Save the Python code above into a file named `main.py`.\")\n",
        "print(\"3. Install FastAPI and Uvicorn: `pip install fastapi uvicorn[standard]`\")\n",
        "print(\"4. Run from your terminal: `uvicorn main:app --reload --port 8000`\")\n",
        "print(\"5. You can then test it, for example, with a curl command in your terminal:\")\n",
        "print(f\"   `curl \\\"http://localhost:8000/recommendations/{test_user_id}?top_n=3\\\"` (replace {test_user_id} if needed)\")\n",
        "print(\"   Or open `http://localhost:8000/docs` in your browser for interactive API documentation (Swagger UI).\")\n",
        "\n",
        "print(\"\\n--- End of Cell 8 ---\")\n",
        "# Display the FastAPI code block so it's easily copyable\n",
        "display(Code(connection_pool_code, language='python'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8_wAiDmAQVXg",
        "outputId": "c09ab069-4269-4ed6-b945-781847be71be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Designing the FastAPI Application for Recommendations ---\n",
            "--- FastAPI Application Code (for main.py) ---\n",
            "\n",
            "To make this API live:\n",
            "1. Replace 'your_postgres_connection_string_here_from_cell_1' with your actual DB_CONNECTION_STRING from Cell 1 (or better, use an environment variable).\n",
            "2. Save the Python code above into a file named `main.py`.\n",
            "3. Install FastAPI and Uvicorn: `pip install fastapi uvicorn[standard]`\n",
            "4. Run from your terminal: `uvicorn main:app --reload --port 8000`\n",
            "5. You can then test it, for example, with a curl command in your terminal:\n",
            "   `curl \"http://localhost:8000/recommendations/1?top_n=3\"` (replace 1 if needed)\n",
            "   Or open `http://localhost:8000/docs` in your browser for interactive API documentation (Swagger UI).\n",
            "\n",
            "--- End of Cell 8 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "# main.py (Illustrative - For local execution with Uvicorn)\n",
              "import os\n",
              "from fastapi import FastAPI, HTTPException\n",
              "import psycopg2\n",
              "from psycopg2.pool import SimpleConnectionPool\n",
              "\n",
              "# --- Configuration ---\n",
              "# In a real app, use environment variables for sensitive info\n",
              "DATABASE_URL = os.environ.get(\"DATABASE_URL\", \"your_postgres_connection_string_here_from_cell_1\")\n",
              "# It's critical that this DATABASE_URL matches your actual DB_CONNECTION_STRING from Cell 1\n",
              "\n",
              "# Initialize a connection pool\n",
              "try:\n",
              "    pool = SimpleConnectionPool(\n",
              "        minconn=2,  # Minimum number of connections in the pool\n",
              "        maxconn=10, # Maximum number of connections\n",
              "        dsn=DATABASE_URL\n",
              "    )\n",
              "    print(\"Database connection pool initialized.\")\n",
              "except Exception as e:\n",
              "    print(f\"FATAL: Could not initialize database connection pool: {e}\")\n",
              "    pool = None # Ensure pool is None if initialization fails\n",
              "\n",
              "app = FastAPI(title=\"PostgresML Recommender API\")\n",
              "\n",
              "@app.on_event(\"startup\")\n",
              "async def startup_event():\n",
              "    if pool is None:\n",
              "        # This will prevent the app from starting if pool is not available\n",
              "        raise RuntimeError(\"Database connection pool is not available. FastAPI app cannot start.\")\n",
              "    print(\"FastAPI application started successfully with DB pool.\")\n",
              "\n",
              "@app.on_event(\"shutdown\")\n",
              "def shutdown_event():\n",
              "    if pool:\n",
              "        pool.closeall()\n",
              "        print(\"Database connection pool closed.\")\n",
              "\n",
              "# --- API Endpoint (as per Part 5, Slide 2) ---\n",
              "@app.get(\"/recommendations/{user_id}\", tags=[\"Recommendations\"])\n",
              "async def get_user_recommendations_api(user_id: int, top_n: int = 5):\n",
              "    '''\n",
              "    Fetches top N product recommendations for a given user_id.\n",
              "    '''\n",
              "    if pool is None:\n",
              "        raise HTTPException(status_code=503, detail=\"Database service unavailable\")\n",
              "\n",
              "    conn = None\n",
              "    try:\n",
              "        conn = pool.getconn() # Get a connection from the pool\n",
              "        with conn.cursor() as cur:\n",
              "            # Call the SQL function created in Cell 7\n",
              "            cur.execute(\"SELECT product_id, product_name, category, predicted_score FROM get_recommendations(%s, %s);\", (user_id, top_n))\n",
              "            recs_data = cur.fetchall()\n",
              "            \n",
              "            if not recs_data:\n",
              "                # Return 404 if user exists but no recommendations, or handle as appropriate\n",
              "                # Check if user exists to differentiate\n",
              "                cur.execute(\"SELECT 1 FROM users WHERE user_id = %s;\", (user_id,))\n",
              "                if not cur.fetchone():\n",
              "                    raise HTTPException(status_code=404, detail=f\"User ID {user_id} not found.\")\n",
              "                return {\"user_id\": user_id, \"recommendations\": []} # User exists, no recs\n",
              "\n",
              "            recommendations = [\n",
              "                {\"product_id\": r[0], \"product_name\": r[1], \"category\": r[2], \"predicted_score\": round(r[3], 4)}\n",
              "                for r in recs_data\n",
              "            ]\n",
              "            return {\"user_id\": user_id, \"top_n_requested\": top_n, \"recommendations\": recommendations}\n",
              "    except psycopg2.Error as db_err:\n",
              "        # Log the database error, return a generic server error\n",
              "        print(f\"Database error for user_id {user_id}: {db_err}\")\n",
              "        raise HTTPException(status_code=500, detail=\"Internal database error\")\n",
              "    except Exception as e:\n",
              "        # Log other errors\n",
              "        print(f\"An unexpected error occurred for user_id {user_id}: {e}\")\n",
              "        raise HTTPException(status_code=500, detail=\"An unexpected internal server error occurred\")\n",
              "    finally:\n",
              "        if conn:\n",
              "            pool.putconn(conn) # Return the connection to the pool\n",
              "\n",
              "# To run this (locally, not in Colab):\n",
              "# 1. Save this code as `main.py`.\n",
              "# 2. Set the DATABASE_URL environment variable: `export DATABASE_URL=\"your_actual_connection_string\"`\n",
              "# 3. Run Uvicorn: `uvicorn main:app --reload --port 8000`\n",
              "# 4. Access in browser or with curl: `curl http://localhost:8000/recommendations/1?top_n=3`"
            ],
            "text/html": [
              "<style>pre { line-height: 125%; }\n",
              "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
              "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
              "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
              "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
              ".output_html .hll { background-color: #ffffcc }\n",
              ".output_html { background: #f8f8f8; }\n",
              ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
              ".output_html .err { border: 1px solid #F00 } /* Error */\n",
              ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
              ".output_html .o { color: #666 } /* Operator */\n",
              ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
              ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
              ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
              ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
              ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
              ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
              ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
              ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
              ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
              ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
              ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
              ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
              ".output_html .go { color: #717171 } /* Generic.Output */\n",
              ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
              ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
              ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
              ".output_html .gt { color: #04D } /* Generic.Traceback */\n",
              ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
              ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
              ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
              ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
              ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
              ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
              ".output_html .m { color: #666 } /* Literal.Number */\n",
              ".output_html .s { color: #BA2121 } /* Literal.String */\n",
              ".output_html .na { color: #687822 } /* Name.Attribute */\n",
              ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
              ".output_html .nc { color: #00F; font-weight: bold } /* Name.Class */\n",
              ".output_html .no { color: #800 } /* Name.Constant */\n",
              ".output_html .nd { color: #A2F } /* Name.Decorator */\n",
              ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
              ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
              ".output_html .nf { color: #00F } /* Name.Function */\n",
              ".output_html .nl { color: #767600 } /* Name.Label */\n",
              ".output_html .nn { color: #00F; font-weight: bold } /* Name.Namespace */\n",
              ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
              ".output_html .nv { color: #19177C } /* Name.Variable */\n",
              ".output_html .ow { color: #A2F; font-weight: bold } /* Operator.Word */\n",
              ".output_html .w { color: #BBB } /* Text.Whitespace */\n",
              ".output_html .mb { color: #666 } /* Literal.Number.Bin */\n",
              ".output_html .mf { color: #666 } /* Literal.Number.Float */\n",
              ".output_html .mh { color: #666 } /* Literal.Number.Hex */\n",
              ".output_html .mi { color: #666 } /* Literal.Number.Integer */\n",
              ".output_html .mo { color: #666 } /* Literal.Number.Oct */\n",
              ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
              ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
              ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
              ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
              ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
              ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
              ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
              ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
              ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
              ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
              ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
              ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
              ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
              ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
              ".output_html .fm { color: #00F } /* Name.Function.Magic */\n",
              ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
              ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
              ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
              ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
              ".output_html .il { color: #666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"c1\"># main.py (Illustrative - For local execution with Uvicorn)</span>\n",
              "<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">os</span>\n",
              "<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">fastapi</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">FastAPI</span><span class=\"p\">,</span> <span class=\"n\">HTTPException</span>\n",
              "<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">psycopg2</span>\n",
              "<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">psycopg2.pool</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">SimpleConnectionPool</span>\n",
              "\n",
              "<span class=\"c1\"># --- Configuration ---</span>\n",
              "<span class=\"c1\"># In a real app, use environment variables for sensitive info</span>\n",
              "<span class=\"n\">DATABASE_URL</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s2\">&quot;DATABASE_URL&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;your_postgres_connection_string_here_from_cell_1&quot;</span><span class=\"p\">)</span>\n",
              "<span class=\"c1\"># It&#39;s critical that this DATABASE_URL matches your actual DB_CONNECTION_STRING from Cell 1</span>\n",
              "\n",
              "<span class=\"c1\"># Initialize a connection pool</span>\n",
              "<span class=\"k\">try</span><span class=\"p\">:</span>\n",
              "    <span class=\"n\">pool</span> <span class=\"o\">=</span> <span class=\"n\">SimpleConnectionPool</span><span class=\"p\">(</span>\n",
              "        <span class=\"n\">minconn</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span>  <span class=\"c1\"># Minimum number of connections in the pool</span>\n",
              "        <span class=\"n\">maxconn</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"c1\"># Maximum number of connections</span>\n",
              "        <span class=\"n\">dsn</span><span class=\"o\">=</span><span class=\"n\">DATABASE_URL</span>\n",
              "    <span class=\"p\">)</span>\n",
              "    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Database connection pool initialized.&quot;</span><span class=\"p\">)</span>\n",
              "<span class=\"k\">except</span> <span class=\"ne\">Exception</span> <span class=\"k\">as</span> <span class=\"n\">e</span><span class=\"p\">:</span>\n",
              "    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;FATAL: Could not initialize database connection pool: </span><span class=\"si\">{</span><span class=\"n\">e</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n",
              "    <span class=\"n\">pool</span> <span class=\"o\">=</span> <span class=\"kc\">None</span> <span class=\"c1\"># Ensure pool is None if initialization fails</span>\n",
              "\n",
              "<span class=\"n\">app</span> <span class=\"o\">=</span> <span class=\"n\">FastAPI</span><span class=\"p\">(</span><span class=\"n\">title</span><span class=\"o\">=</span><span class=\"s2\">&quot;PostgresML Recommender API&quot;</span><span class=\"p\">)</span>\n",
              "\n",
              "<span class=\"nd\">@app</span><span class=\"o\">.</span><span class=\"n\">on_event</span><span class=\"p\">(</span><span class=\"s2\">&quot;startup&quot;</span><span class=\"p\">)</span>\n",
              "<span class=\"k\">async</span> <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">startup_event</span><span class=\"p\">():</span>\n",
              "    <span class=\"k\">if</span> <span class=\"n\">pool</span> <span class=\"ow\">is</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n",
              "        <span class=\"c1\"># This will prevent the app from starting if pool is not available</span>\n",
              "        <span class=\"k\">raise</span> <span class=\"ne\">RuntimeError</span><span class=\"p\">(</span><span class=\"s2\">&quot;Database connection pool is not available. FastAPI app cannot start.&quot;</span><span class=\"p\">)</span>\n",
              "    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;FastAPI application started successfully with DB pool.&quot;</span><span class=\"p\">)</span>\n",
              "\n",
              "<span class=\"nd\">@app</span><span class=\"o\">.</span><span class=\"n\">on_event</span><span class=\"p\">(</span><span class=\"s2\">&quot;shutdown&quot;</span><span class=\"p\">)</span>\n",
              "<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">shutdown_event</span><span class=\"p\">():</span>\n",
              "    <span class=\"k\">if</span> <span class=\"n\">pool</span><span class=\"p\">:</span>\n",
              "        <span class=\"n\">pool</span><span class=\"o\">.</span><span class=\"n\">closeall</span><span class=\"p\">()</span>\n",
              "        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Database connection pool closed.&quot;</span><span class=\"p\">)</span>\n",
              "\n",
              "<span class=\"c1\"># --- API Endpoint (as per Part 5, Slide 2) ---</span>\n",
              "<span class=\"nd\">@app</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s2\">&quot;/recommendations/</span><span class=\"si\">{user_id}</span><span class=\"s2\">&quot;</span><span class=\"p\">,</span> <span class=\"n\">tags</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">&quot;Recommendations&quot;</span><span class=\"p\">])</span>\n",
              "<span class=\"k\">async</span> <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">get_user_recommendations_api</span><span class=\"p\">(</span><span class=\"n\">user_id</span><span class=\"p\">:</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">top_n</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">5</span><span class=\"p\">):</span>\n",
              "<span class=\"w\">    </span><span class=\"sd\">&#39;&#39;&#39;</span>\n",
              "<span class=\"sd\">    Fetches top N product recommendations for a given user_id.</span>\n",
              "<span class=\"sd\">    &#39;&#39;&#39;</span>\n",
              "    <span class=\"k\">if</span> <span class=\"n\">pool</span> <span class=\"ow\">is</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n",
              "        <span class=\"k\">raise</span> <span class=\"n\">HTTPException</span><span class=\"p\">(</span><span class=\"n\">status_code</span><span class=\"o\">=</span><span class=\"mi\">503</span><span class=\"p\">,</span> <span class=\"n\">detail</span><span class=\"o\">=</span><span class=\"s2\">&quot;Database service unavailable&quot;</span><span class=\"p\">)</span>\n",
              "\n",
              "    <span class=\"n\">conn</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n",
              "    <span class=\"k\">try</span><span class=\"p\">:</span>\n",
              "        <span class=\"n\">conn</span> <span class=\"o\">=</span> <span class=\"n\">pool</span><span class=\"o\">.</span><span class=\"n\">getconn</span><span class=\"p\">()</span> <span class=\"c1\"># Get a connection from the pool</span>\n",
              "        <span class=\"k\">with</span> <span class=\"n\">conn</span><span class=\"o\">.</span><span class=\"n\">cursor</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">cur</span><span class=\"p\">:</span>\n",
              "            <span class=\"c1\"># Call the SQL function created in Cell 7</span>\n",
              "            <span class=\"n\">cur</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"s2\">&quot;SELECT product_id, product_name, category, predicted_score FROM get_recommendations(</span><span class=\"si\">%s</span><span class=\"s2\">, </span><span class=\"si\">%s</span><span class=\"s2\">);&quot;</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">user_id</span><span class=\"p\">,</span> <span class=\"n\">top_n</span><span class=\"p\">))</span>\n",
              "            <span class=\"n\">recs_data</span> <span class=\"o\">=</span> <span class=\"n\">cur</span><span class=\"o\">.</span><span class=\"n\">fetchall</span><span class=\"p\">()</span>\n",
              "            \n",
              "            <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">recs_data</span><span class=\"p\">:</span>\n",
              "                <span class=\"c1\"># Return 404 if user exists but no recommendations, or handle as appropriate</span>\n",
              "                <span class=\"c1\"># Check if user exists to differentiate</span>\n",
              "                <span class=\"n\">cur</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"p\">(</span><span class=\"s2\">&quot;SELECT 1 FROM users WHERE user_id = </span><span class=\"si\">%s</span><span class=\"s2\">;&quot;</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">user_id</span><span class=\"p\">,))</span>\n",
              "                <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">cur</span><span class=\"o\">.</span><span class=\"n\">fetchone</span><span class=\"p\">():</span>\n",
              "                    <span class=\"k\">raise</span> <span class=\"n\">HTTPException</span><span class=\"p\">(</span><span class=\"n\">status_code</span><span class=\"o\">=</span><span class=\"mi\">404</span><span class=\"p\">,</span> <span class=\"n\">detail</span><span class=\"o\">=</span><span class=\"sa\">f</span><span class=\"s2\">&quot;User ID </span><span class=\"si\">{</span><span class=\"n\">user_id</span><span class=\"si\">}</span><span class=\"s2\"> not found.&quot;</span><span class=\"p\">)</span>\n",
              "                <span class=\"k\">return</span> <span class=\"p\">{</span><span class=\"s2\">&quot;user_id&quot;</span><span class=\"p\">:</span> <span class=\"n\">user_id</span><span class=\"p\">,</span> <span class=\"s2\">&quot;recommendations&quot;</span><span class=\"p\">:</span> <span class=\"p\">[]}</span> <span class=\"c1\"># User exists, no recs</span>\n",
              "\n",
              "            <span class=\"n\">recommendations</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n",
              "                <span class=\"p\">{</span><span class=\"s2\">&quot;product_id&quot;</span><span class=\"p\">:</span> <span class=\"n\">r</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"s2\">&quot;product_name&quot;</span><span class=\"p\">:</span> <span class=\"n\">r</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"s2\">&quot;category&quot;</span><span class=\"p\">:</span> <span class=\"n\">r</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"s2\">&quot;predicted_score&quot;</span><span class=\"p\">:</span> <span class=\"nb\">round</span><span class=\"p\">(</span><span class=\"n\">r</span><span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">],</span> <span class=\"mi\">4</span><span class=\"p\">)}</span>\n",
              "                <span class=\"k\">for</span> <span class=\"n\">r</span> <span class=\"ow\">in</span> <span class=\"n\">recs_data</span>\n",
              "            <span class=\"p\">]</span>\n",
              "            <span class=\"k\">return</span> <span class=\"p\">{</span><span class=\"s2\">&quot;user_id&quot;</span><span class=\"p\">:</span> <span class=\"n\">user_id</span><span class=\"p\">,</span> <span class=\"s2\">&quot;top_n_requested&quot;</span><span class=\"p\">:</span> <span class=\"n\">top_n</span><span class=\"p\">,</span> <span class=\"s2\">&quot;recommendations&quot;</span><span class=\"p\">:</span> <span class=\"n\">recommendations</span><span class=\"p\">}</span>\n",
              "    <span class=\"k\">except</span> <span class=\"n\">psycopg2</span><span class=\"o\">.</span><span class=\"n\">Error</span> <span class=\"k\">as</span> <span class=\"n\">db_err</span><span class=\"p\">:</span>\n",
              "        <span class=\"c1\"># Log the database error, return a generic server error</span>\n",
              "        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Database error for user_id </span><span class=\"si\">{</span><span class=\"n\">user_id</span><span class=\"si\">}</span><span class=\"s2\">: </span><span class=\"si\">{</span><span class=\"n\">db_err</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n",
              "        <span class=\"k\">raise</span> <span class=\"n\">HTTPException</span><span class=\"p\">(</span><span class=\"n\">status_code</span><span class=\"o\">=</span><span class=\"mi\">500</span><span class=\"p\">,</span> <span class=\"n\">detail</span><span class=\"o\">=</span><span class=\"s2\">&quot;Internal database error&quot;</span><span class=\"p\">)</span>\n",
              "    <span class=\"k\">except</span> <span class=\"ne\">Exception</span> <span class=\"k\">as</span> <span class=\"n\">e</span><span class=\"p\">:</span>\n",
              "        <span class=\"c1\"># Log other errors</span>\n",
              "        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;An unexpected error occurred for user_id </span><span class=\"si\">{</span><span class=\"n\">user_id</span><span class=\"si\">}</span><span class=\"s2\">: </span><span class=\"si\">{</span><span class=\"n\">e</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n",
              "        <span class=\"k\">raise</span> <span class=\"n\">HTTPException</span><span class=\"p\">(</span><span class=\"n\">status_code</span><span class=\"o\">=</span><span class=\"mi\">500</span><span class=\"p\">,</span> <span class=\"n\">detail</span><span class=\"o\">=</span><span class=\"s2\">&quot;An unexpected internal server error occurred&quot;</span><span class=\"p\">)</span>\n",
              "    <span class=\"k\">finally</span><span class=\"p\">:</span>\n",
              "        <span class=\"k\">if</span> <span class=\"n\">conn</span><span class=\"p\">:</span>\n",
              "            <span class=\"n\">pool</span><span class=\"o\">.</span><span class=\"n\">putconn</span><span class=\"p\">(</span><span class=\"n\">conn</span><span class=\"p\">)</span> <span class=\"c1\"># Return the connection to the pool</span>\n",
              "\n",
              "<span class=\"c1\"># To run this (locally, not in Colab):</span>\n",
              "<span class=\"c1\"># 1. Save this code as `main.py`.</span>\n",
              "<span class=\"c1\"># 2. Set the DATABASE_URL environment variable: `export DATABASE_URL=&quot;your_actual_connection_string&quot;`</span>\n",
              "<span class=\"c1\"># 3. Run Uvicorn: `uvicorn main:app --reload --port 8000`</span>\n",
              "<span class=\"c1\"># 4. Access in browser or with curl: `curl http://localhost:8000/recommendations/1?top_n=3`</span>\n",
              "</pre></div>\n"
            ],
            "text/latex": "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n\\PY{c+c1}{\\PYZsh{} main.py (Illustrative \\PYZhy{} For local execution with Uvicorn)}\n\\PY{k+kn}{import}\\PY{+w}{ }\\PY{n+nn}{os}\n\\PY{k+kn}{from}\\PY{+w}{ }\\PY{n+nn}{fastapi}\\PY{+w}{ }\\PY{k+kn}{import} \\PY{n}{FastAPI}\\PY{p}{,} \\PY{n}{HTTPException}\n\\PY{k+kn}{import}\\PY{+w}{ }\\PY{n+nn}{psycopg2}\n\\PY{k+kn}{from}\\PY{+w}{ }\\PY{n+nn}{psycopg2}\\PY{n+nn}{.}\\PY{n+nn}{pool}\\PY{+w}{ }\\PY{k+kn}{import} \\PY{n}{SimpleConnectionPool}\n\n\\PY{c+c1}{\\PYZsh{} \\PYZhy{}\\PYZhy{}\\PYZhy{} Configuration \\PYZhy{}\\PYZhy{}\\PYZhy{}}\n\\PY{c+c1}{\\PYZsh{} In a real app, use environment variables for sensitive info}\n\\PY{n}{DATABASE\\PYZus{}URL} \\PY{o}{=} \\PY{n}{os}\\PY{o}{.}\\PY{n}{environ}\\PY{o}{.}\\PY{n}{get}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{DATABASE\\PYZus{}URL}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{your\\PYZus{}postgres\\PYZus{}connection\\PYZus{}string\\PYZus{}here\\PYZus{}from\\PYZus{}cell\\PYZus{}1}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n\\PY{c+c1}{\\PYZsh{} It\\PYZsq{}s critical that this DATABASE\\PYZus{}URL matches your actual DB\\PYZus{}CONNECTION\\PYZus{}STRING from Cell 1}\n\n\\PY{c+c1}{\\PYZsh{} Initialize a connection pool}\n\\PY{k}{try}\\PY{p}{:}\n    \\PY{n}{pool} \\PY{o}{=} \\PY{n}{SimpleConnectionPool}\\PY{p}{(}\n        \\PY{n}{minconn}\\PY{o}{=}\\PY{l+m+mi}{2}\\PY{p}{,}  \\PY{c+c1}{\\PYZsh{} Minimum number of connections in the pool}\n        \\PY{n}{maxconn}\\PY{o}{=}\\PY{l+m+mi}{10}\\PY{p}{,} \\PY{c+c1}{\\PYZsh{} Maximum number of connections}\n        \\PY{n}{dsn}\\PY{o}{=}\\PY{n}{DATABASE\\PYZus{}URL}\n    \\PY{p}{)}\n    \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Database connection pool initialized.}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n\\PY{k}{except} \\PY{n+ne}{Exception} \\PY{k}{as} \\PY{n}{e}\\PY{p}{:}\n    \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{FATAL: Could not initialize database connection pool: }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{e}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n    \\PY{n}{pool} \\PY{o}{=} \\PY{k+kc}{None} \\PY{c+c1}{\\PYZsh{} Ensure pool is None if initialization fails}\n\n\\PY{n}{app} \\PY{o}{=} \\PY{n}{FastAPI}\\PY{p}{(}\\PY{n}{title}\\PY{o}{=}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{PostgresML Recommender API}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n\n\\PY{n+nd}{@app}\\PY{o}{.}\\PY{n}{on\\PYZus{}event}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{startup}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n\\PY{k}{async} \\PY{k}{def}\\PY{+w}{ }\\PY{n+nf}{startup\\PYZus{}event}\\PY{p}{(}\\PY{p}{)}\\PY{p}{:}\n    \\PY{k}{if} \\PY{n}{pool} \\PY{o+ow}{is} \\PY{k+kc}{None}\\PY{p}{:}\n        \\PY{c+c1}{\\PYZsh{} This will prevent the app from starting if pool is not available}\n        \\PY{k}{raise} \\PY{n+ne}{RuntimeError}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Database connection pool is not available. FastAPI app cannot start.}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n    \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{FastAPI application started successfully with DB pool.}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n\n\\PY{n+nd}{@app}\\PY{o}{.}\\PY{n}{on\\PYZus{}event}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{shutdown}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n\\PY{k}{def}\\PY{+w}{ }\\PY{n+nf}{shutdown\\PYZus{}event}\\PY{p}{(}\\PY{p}{)}\\PY{p}{:}\n    \\PY{k}{if} \\PY{n}{pool}\\PY{p}{:}\n        \\PY{n}{pool}\\PY{o}{.}\\PY{n}{closeall}\\PY{p}{(}\\PY{p}{)}\n        \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Database connection pool closed.}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n\n\\PY{c+c1}{\\PYZsh{} \\PYZhy{}\\PYZhy{}\\PYZhy{} API Endpoint (as per Part 5, Slide 2) \\PYZhy{}\\PYZhy{}\\PYZhy{}}\n\\PY{n+nd}{@app}\\PY{o}{.}\\PY{n}{get}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{/recommendations/}\\PY{l+s+si}{\\PYZob{}user\\PYZus{}id\\PYZcb{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{,} \\PY{n}{tags}\\PY{o}{=}\\PY{p}{[}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Recommendations}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\\PY{p}{)}\n\\PY{k}{async} \\PY{k}{def}\\PY{+w}{ }\\PY{n+nf}{get\\PYZus{}user\\PYZus{}recommendations\\PYZus{}api}\\PY{p}{(}\\PY{n}{user\\PYZus{}id}\\PY{p}{:} \\PY{n+nb}{int}\\PY{p}{,} \\PY{n}{top\\PYZus{}n}\\PY{p}{:} \\PY{n+nb}{int} \\PY{o}{=} \\PY{l+m+mi}{5}\\PY{p}{)}\\PY{p}{:}\n\\PY{+w}{    }\\PY{l+s+sd}{\\PYZsq{}\\PYZsq{}\\PYZsq{}}\n\\PY{l+s+sd}{    Fetches top N product recommendations for a given user\\PYZus{}id.}\n\\PY{l+s+sd}{    \\PYZsq{}\\PYZsq{}\\PYZsq{}}\n    \\PY{k}{if} \\PY{n}{pool} \\PY{o+ow}{is} \\PY{k+kc}{None}\\PY{p}{:}\n        \\PY{k}{raise} \\PY{n}{HTTPException}\\PY{p}{(}\\PY{n}{status\\PYZus{}code}\\PY{o}{=}\\PY{l+m+mi}{503}\\PY{p}{,} \\PY{n}{detail}\\PY{o}{=}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Database service unavailable}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n\n    \\PY{n}{conn} \\PY{o}{=} \\PY{k+kc}{None}\n    \\PY{k}{try}\\PY{p}{:}\n        \\PY{n}{conn} \\PY{o}{=} \\PY{n}{pool}\\PY{o}{.}\\PY{n}{getconn}\\PY{p}{(}\\PY{p}{)} \\PY{c+c1}{\\PYZsh{} Get a connection from the pool}\n        \\PY{k}{with} \\PY{n}{conn}\\PY{o}{.}\\PY{n}{cursor}\\PY{p}{(}\\PY{p}{)} \\PY{k}{as} \\PY{n}{cur}\\PY{p}{:}\n            \\PY{c+c1}{\\PYZsh{} Call the SQL function created in Cell 7}\n            \\PY{n}{cur}\\PY{o}{.}\\PY{n}{execute}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{SELECT product\\PYZus{}id, product\\PYZus{}name, category, predicted\\PYZus{}score FROM get\\PYZus{}recommendations(}\\PY{l+s+si}{\\PYZpc{}s}\\PY{l+s+s2}{, }\\PY{l+s+si}{\\PYZpc{}s}\\PY{l+s+s2}{);}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{,} \\PY{p}{(}\\PY{n}{user\\PYZus{}id}\\PY{p}{,} \\PY{n}{top\\PYZus{}n}\\PY{p}{)}\\PY{p}{)}\n            \\PY{n}{recs\\PYZus{}data} \\PY{o}{=} \\PY{n}{cur}\\PY{o}{.}\\PY{n}{fetchall}\\PY{p}{(}\\PY{p}{)}\n            \n            \\PY{k}{if} \\PY{o+ow}{not} \\PY{n}{recs\\PYZus{}data}\\PY{p}{:}\n                \\PY{c+c1}{\\PYZsh{} Return 404 if user exists but no recommendations, or handle as appropriate}\n                \\PY{c+c1}{\\PYZsh{} Check if user exists to differentiate}\n                \\PY{n}{cur}\\PY{o}{.}\\PY{n}{execute}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{SELECT 1 FROM users WHERE user\\PYZus{}id = }\\PY{l+s+si}{\\PYZpc{}s}\\PY{l+s+s2}{;}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{,} \\PY{p}{(}\\PY{n}{user\\PYZus{}id}\\PY{p}{,}\\PY{p}{)}\\PY{p}{)}\n                \\PY{k}{if} \\PY{o+ow}{not} \\PY{n}{cur}\\PY{o}{.}\\PY{n}{fetchone}\\PY{p}{(}\\PY{p}{)}\\PY{p}{:}\n                    \\PY{k}{raise} \\PY{n}{HTTPException}\\PY{p}{(}\\PY{n}{status\\PYZus{}code}\\PY{o}{=}\\PY{l+m+mi}{404}\\PY{p}{,} \\PY{n}{detail}\\PY{o}{=}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{User ID }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{user\\PYZus{}id}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{ not found.}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n                \\PY{k}{return} \\PY{p}{\\PYZob{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{user\\PYZus{}id}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{n}{user\\PYZus{}id}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{recommendations}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{p}{[}\\PY{p}{]}\\PY{p}{\\PYZcb{}} \\PY{c+c1}{\\PYZsh{} User exists, no recs}\n\n            \\PY{n}{recommendations} \\PY{o}{=} \\PY{p}{[}\n                \\PY{p}{\\PYZob{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{product\\PYZus{}id}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{n}{r}\\PY{p}{[}\\PY{l+m+mi}{0}\\PY{p}{]}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{product\\PYZus{}name}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{n}{r}\\PY{p}{[}\\PY{l+m+mi}{1}\\PY{p}{]}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{category}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{n}{r}\\PY{p}{[}\\PY{l+m+mi}{2}\\PY{p}{]}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{predicted\\PYZus{}score}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{n+nb}{round}\\PY{p}{(}\\PY{n}{r}\\PY{p}{[}\\PY{l+m+mi}{3}\\PY{p}{]}\\PY{p}{,} \\PY{l+m+mi}{4}\\PY{p}{)}\\PY{p}{\\PYZcb{}}\n                \\PY{k}{for} \\PY{n}{r} \\PY{o+ow}{in} \\PY{n}{recs\\PYZus{}data}\n            \\PY{p}{]}\n            \\PY{k}{return} \\PY{p}{\\PYZob{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{user\\PYZus{}id}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{n}{user\\PYZus{}id}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{top\\PYZus{}n\\PYZus{}requested}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{n}{top\\PYZus{}n}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{recommendations}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{n}{recommendations}\\PY{p}{\\PYZcb{}}\n    \\PY{k}{except} \\PY{n}{psycopg2}\\PY{o}{.}\\PY{n}{Error} \\PY{k}{as} \\PY{n}{db\\PYZus{}err}\\PY{p}{:}\n        \\PY{c+c1}{\\PYZsh{} Log the database error, return a generic server error}\n        \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Database error for user\\PYZus{}id }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{user\\PYZus{}id}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{: }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{db\\PYZus{}err}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n        \\PY{k}{raise} \\PY{n}{HTTPException}\\PY{p}{(}\\PY{n}{status\\PYZus{}code}\\PY{o}{=}\\PY{l+m+mi}{500}\\PY{p}{,} \\PY{n}{detail}\\PY{o}{=}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Internal database error}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n    \\PY{k}{except} \\PY{n+ne}{Exception} \\PY{k}{as} \\PY{n}{e}\\PY{p}{:}\n        \\PY{c+c1}{\\PYZsh{} Log other errors}\n        \\PY{n+nb}{print}\\PY{p}{(}\\PY{l+s+sa}{f}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{An unexpected error occurred for user\\PYZus{}id }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{user\\PYZus{}id}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{: }\\PY{l+s+si}{\\PYZob{}}\\PY{n}{e}\\PY{l+s+si}{\\PYZcb{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n        \\PY{k}{raise} \\PY{n}{HTTPException}\\PY{p}{(}\\PY{n}{status\\PYZus{}code}\\PY{o}{=}\\PY{l+m+mi}{500}\\PY{p}{,} \\PY{n}{detail}\\PY{o}{=}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{An unexpected internal server error occurred}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n    \\PY{k}{finally}\\PY{p}{:}\n        \\PY{k}{if} \\PY{n}{conn}\\PY{p}{:}\n            \\PY{n}{pool}\\PY{o}{.}\\PY{n}{putconn}\\PY{p}{(}\\PY{n}{conn}\\PY{p}{)} \\PY{c+c1}{\\PYZsh{} Return the connection to the pool}\n\n\\PY{c+c1}{\\PYZsh{} To run this (locally, not in Colab):}\n\\PY{c+c1}{\\PYZsh{} 1. Save this code as `main.py`.}\n\\PY{c+c1}{\\PYZsh{} 2. Set the DATABASE\\PYZus{}URL environment variable: `export DATABASE\\PYZus{}URL=\\PYZdq{}your\\PYZus{}actual\\PYZus{}connection\\PYZus{}string\\PYZdq{}`}\n\\PY{c+c1}{\\PYZsh{} 3. Run Uvicorn: `uvicorn main:app \\PYZhy{}\\PYZhy{}reload \\PYZhy{}\\PYZhy{}port 8000`}\n\\PY{c+c1}{\\PYZsh{} 4. Access in browser or with curl: `curl http://localhost:8000/recommendations/1?top\\PYZus{}n=3`}\n\\end{Verbatim}\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 9: PRODUCTION CONSIDERATIONS - MONITORING & SECURITY (SQL)\n",
        "#@markdown Implements basic SQL constructs for monitoring predictions and setting up database permissions.\n",
        "print(\"--- Production Considerations: Basic Monitoring & Security ---\")\n",
        "\n",
        "# --- Basic Monitoring (as per Part 4, Slide 1) ---\n",
        "print(\"\\nSetting up basic monitoring (schema, table, view)...\")\n",
        "run_sql(\"CREATE SCHEMA IF NOT EXISTS monitoring;\", fetch_results=False)\n",
        "monitoring_table_sql = \"\"\"\n",
        "CREATE TABLE IF NOT EXISTS monitoring.predictions (\n",
        "    id SERIAL PRIMARY KEY, model_name TEXT, user_id_requested INTEGER,\n",
        "    num_recommendations_generated INTEGER, first_recommendation_score FLOAT,\n",
        "    response_time_ms FLOAT, timestamp TIMESTAMP DEFAULT NOW()\n",
        ");\"\"\"\n",
        "run_sql(monitoring_table_sql, fetch_results=False)\n",
        "print(\"✅ Table 'monitoring.predictions' created (if not exists).\")\n",
        "\n",
        "monitoring_view_sql = \"\"\"\n",
        "CREATE OR REPLACE VIEW monitoring.metrics_view AS\n",
        "SELECT model_name, COUNT(*) as total_prediction_calls,\n",
        "       AVG(response_time_ms) as avg_response_time_ms,\n",
        "       AVG(num_recommendations_generated) as avg_recs_generated,\n",
        "       MIN(timestamp) as first_call_at, MAX(timestamp) as last_call_at\n",
        "FROM monitoring.predictions GROUP BY model_name;\"\"\"\n",
        "run_sql(monitoring_view_sql, fetch_results=False)\n",
        "print(\"✅ View 'monitoring.metrics_view' created/recreated.\")\n",
        "\n",
        "print(\"\\nSimulating logging of prediction calls to 'monitoring.predictions'...\")\n",
        "log_sim_sql = \"INSERT INTO monitoring.predictions (model_name, user_id_requested, num_recommendations_generated, first_recommendation_score, response_time_ms) VALUES (%s, %s, %s, %s, %s);\"\n",
        "# Fetch a sample score IF model worked and mock is False, otherwise use a dummy score for logging\n",
        "sample_recs_for_log_df = pd.DataFrame() # Initialize\n",
        "if 'MOCK_MODEL_PREDICTIONS' in globals() and not MOCK_MODEL_PREDICTIONS and 'test_user_id' in globals():\n",
        "    sample_recs_for_log_df = run_sql(f\"SELECT predicted_score FROM get_recommendations({test_user_id}, 1) LIMIT 1;\", quiet=True)\n",
        "\n",
        "top_score_for_log = None\n",
        "if not sample_recs_for_log_df.empty and 'predicted_score' in sample_recs_for_log_df.columns and sample_recs_for_log_df.iloc[0]['predicted_score'] is not None:\n",
        "    top_score_for_log = sample_recs_for_log_df.iloc[0]['predicted_score']\n",
        "else: # Fallback if no real score (mock mode or error)\n",
        "    top_score_for_log = round(random.uniform(2.5, 4.5), 2) if 'random' in globals() else 3.5\n",
        "\n",
        "\n",
        "dummy_logs_data = [\n",
        "    (PROJECT_NAME if 'PROJECT_NAME' in globals() else 'product_recommender_v1', test_user_id if 'test_user_id' in globals() else 1, 5, top_score_for_log, random.uniform(50, 200)),\n",
        "    (PROJECT_NAME if 'PROJECT_NAME' in globals() else 'product_recommender_v1', random.randint(1, NUM_USERS if 'NUM_USERS' in globals() else 100), 3, round(random.uniform(1,5),2), random.uniform(40, 150)),\n",
        "    (PROJECT_NAME if 'PROJECT_NAME' in globals() else 'product_recommender_v1', random.randint(1, NUM_USERS if 'NUM_USERS' in globals() else 100), 5, round(random.uniform(1,5),2), random.uniform(60, 250))\n",
        "]\n",
        "for log_entry in dummy_logs_data: run_sql(log_sim_sql, log_entry, fetch_results=False, quiet=True)\n",
        "print(f\"✅ Inserted {len(dummy_logs_data)} dummy log entries.\")\n",
        "\n",
        "print(\"\\n--- Current Monitoring Metrics ---\")\n",
        "metrics_df = run_sql(\"SELECT * FROM monitoring.metrics_view;\")\n",
        "if not metrics_df.empty: print(metrics_df.to_string())\n",
        "else: print(\"No metrics available yet in 'monitoring.metrics_view'.\")\n",
        "\n",
        "\n",
        "# --- Essential Security (as per Part 4, Slide 2 - MODIFIED) ---\n",
        "API_ROLE_NAME = \"recommender_api_user\" #@param {type:\"string\"}\n",
        "print(f\"\\n--- Configuring Permissions for hypothetical role: '{API_ROLE_NAME}' ---\")\n",
        "print(f\"NOTE: Actual 'CREATE ROLE {API_ROLE_NAME};' might be restricted in this environment.\")\n",
        "print(f\"       We will proceed to show the GRANT statements that *would* be applied.\")\n",
        "\n",
        "# Attempt to create role, but expect it might fail due to permissions.\n",
        "# We won't stop the script if it fails, just note it.\n",
        "print(f\"\\nAttempting to create role '{API_ROLE_NAME}' (may fail due to DB user permissions)...\")\n",
        "try:\n",
        "    # Try dropping first in case it exists from a run where user HAD perms, to avoid \"already exists\"\n",
        "    run_sql(f\"DROP ROLE IF EXISTS {API_ROLE_NAME};\", fetch_results=False, quiet=True)\n",
        "    run_sql(f\"CREATE ROLE {API_ROLE_NAME};\", fetch_results=False, quiet=False) # Not quiet to see error\n",
        "    print(f\"✅ INFO: 'CREATE ROLE {API_ROLE_NAME}' statement executed. If no error above, it was successful. If 'permission denied', the role was not created by this script.\")\n",
        "except Exception as e_create_role: # run_sql already prints, but this is an extra catch\n",
        "    print(f\"ℹ️ INFO: Could not create role '{API_ROLE_NAME}' due to: {e_create_role}. This is expected if current DB user lacks CREATEROLE privilege.\")\n",
        "\n",
        "# Grant necessary permissions for the get_recommendations function\n",
        "# Corrected signature for get_recommendations: (INTEGER, INTEGER, BOOLEAN)\n",
        "grant_execute_sql = f\"GRANT EXECUTE ON FUNCTION get_recommendations(INTEGER, INTEGER, BOOLEAN) TO {API_ROLE_NAME};\"\n",
        "print(f\"\\nAttempting: {grant_execute_sql}\")\n",
        "try:\n",
        "    run_sql(grant_execute_sql, fetch_results=False, quiet=False) # Not quiet\n",
        "    print(f\"✅ Statement 'GRANT EXECUTE ON get_recommendations(INTEGER, INTEGER, BOOLEAN)' sent. If role '{API_ROLE_NAME}' exists and current user has grant option, this should succeed.\")\n",
        "except Exception as e_grant_func:\n",
        "    print(f\"ℹ️ INFO: Could not grant EXECUTE on function: {e_grant_func}. Role might not exist or current user lacks grant option.\")\n",
        "\n",
        "\n",
        "print(f\"\\nAttempting to GRANT SELECT on necessary tables to '{API_ROLE_NAME}'...\")\n",
        "tables_for_api_select = [\"users\", \"products\", \"user_features\", \"product_features\", \"interactions\"]\n",
        "all_grants_successful = True\n",
        "for table_name in tables_for_api_select:\n",
        "    grant_select_sql = f\"GRANT SELECT ON TABLE {table_name} TO {API_ROLE_NAME};\"\n",
        "    try:\n",
        "        run_sql(grant_select_sql, fetch_results=False, quiet=True) # Quiet for loop\n",
        "    except Exception as e_grant_table:\n",
        "        print(f\"ℹ️ INFO: Could not grant SELECT on {table_name} to {API_ROLE_NAME}: {e_grant_table}\")\n",
        "        all_grants_successful = False\n",
        "if all_grants_successful:\n",
        "    print(f\"✅ Statements to GRANT SELECT on tables ({', '.join(tables_for_api_select)}) sent.\")\n",
        "\n",
        "print(f\"\\nAttempting to GRANT USAGE on schema public to '{API_ROLE_NAME}'...\")\n",
        "try:\n",
        "    run_sql(f\"GRANT USAGE ON SCHEMA public TO {API_ROLE_NAME};\", fetch_results=False, quiet=True)\n",
        "    print(f\"✅ Statement 'GRANT USAGE ON SCHEMA public' sent.\")\n",
        "except Exception as e_grant_schema:\n",
        "     print(f\"ℹ️ INFO: Could not grant USAGE on schema public: {e_grant_schema}\")\n",
        "\n",
        "\n",
        "print(f\"\\nSecurity Configuration Summary for role '{API_ROLE_NAME}':\")\n",
        "print(f\"- The above GRANT statements define the necessary permissions.\")\n",
        "print(f\"- If role creation failed due to permissions, these grants would also fail or have no effect until the role is manually created by an admin.\")\n",
        "print(f\"- For API connection: role '{API_ROLE_NAME}' would need LOGIN privilege and a password, or other authentication method.\")\n",
        "\n",
        "print(\"\\n--- End of Cell 9 ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oKB9xUyQW14",
        "outputId": "44a05eb4-045a-4e18-c6ce-14f1271d9bc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Production Considerations: Basic Monitoring & Security ---\n",
            "\n",
            "Setting up basic monitoring (schema, table, view)...\n",
            " Kicking off run_sql\n",
            "  Connecting to DB...\n",
            "  Connection successful. Executing query...\n",
            "  Query executed.\n",
            "  Committed (if applicable).\n",
            "  Database connection closed.\n",
            " Kicking off run_sql\n",
            "  Connecting to DB...\n",
            "  Connection successful. Executing query...\n",
            "  Query executed.\n",
            "  Committed (if applicable).\n",
            "  Database connection closed.\n",
            "✅ Table 'monitoring.predictions' created (if not exists).\n",
            " Kicking off run_sql\n",
            "  Connecting to DB...\n",
            "  Connection successful. Executing query...\n",
            "  Query executed.\n",
            "  Committed (if applicable).\n",
            "  Database connection closed.\n",
            "✅ View 'monitoring.metrics_view' created/recreated.\n",
            "\n",
            "Simulating logging of prediction calls to 'monitoring.predictions'...\n",
            "✅ Inserted 3 dummy log entries.\n",
            "\n",
            "--- Current Monitoring Metrics ---\n",
            " Kicking off run_sql\n",
            "  Connecting to DB...\n",
            "  Connection successful. Executing query...\n",
            "  Query executed.\n",
            "  Fetched 1 rows.\n",
            "  Database connection closed.\n",
            "               model_name  total_prediction_calls  avg_response_time_ms  avg_recs_generated              first_call_at               last_call_at\n",
            "0  product_recommender_v1                       5             80.016545  4.2000000000000000 2025-05-15 00:03:23.604045 2025-05-15 00:05:32.745436\n",
            "\n",
            "--- Configuring Permissions for hypothetical role: 'recommender_api_user' ---\n",
            "NOTE: Actual 'CREATE ROLE recommender_api_user;' might be restricted in this environment.\n",
            "       We will proceed to show the GRANT statements that *would* be applied.\n",
            "\n",
            "Attempting to create role 'recommender_api_user' (may fail due to DB user permissions)...\n",
            " Kicking off run_sql\n",
            "  Connecting to DB...\n",
            "  Connection successful. Executing query...\n",
            "🔴 SQL Error in run_sql:\n",
            "   Query: CREATE ROLE recommender_api_user;\n",
            "   Error Code: 42501\n",
            "   Raw pgerror: ERROR:  permission denied to create role\n",
            "\n",
            "   Primary Message: permission denied to create role\n",
            "   Detail Message: None\n",
            "   Hint: None\n",
            "   Full Python Error: permission denied to create role\n",
            "\n",
            "  Database connection closed.\n",
            "✅ INFO: 'CREATE ROLE recommender_api_user' statement executed. If no error above, it was successful. If 'permission denied', the role was not created by this script.\n",
            "\n",
            "Attempting: GRANT EXECUTE ON FUNCTION get_recommendations(INTEGER, INTEGER, BOOLEAN) TO recommender_api_user;\n",
            " Kicking off run_sql\n",
            "  Connecting to DB...\n",
            "  Connection successful. Executing query...\n",
            "🔴 SQL Error in run_sql:\n",
            "   Query: GRANT EXECUTE ON FUNCTION get_recommendations(INTEGER, INTEGER, BOOLEAN) TO recommender_api_user;\n",
            "   Error Code: 42704\n",
            "   Raw pgerror: ERROR:  role \"recommender_api_user\" does not exist\n",
            "\n",
            "   Primary Message: role \"recommender_api_user\" does not exist\n",
            "   Detail Message: None\n",
            "   Hint: None\n",
            "   Full Python Error: role \"recommender_api_user\" does not exist\n",
            "\n",
            "  Database connection closed.\n",
            "✅ Statement 'GRANT EXECUTE ON get_recommendations(INTEGER, INTEGER, BOOLEAN)' sent. If role 'recommender_api_user' exists and current user has grant option, this should succeed.\n",
            "\n",
            "Attempting to GRANT SELECT on necessary tables to 'recommender_api_user'...\n",
            "✅ Statements to GRANT SELECT on tables (users, products, user_features, product_features, interactions) sent.\n",
            "\n",
            "Attempting to GRANT USAGE on schema public to 'recommender_api_user'...\n",
            "✅ Statement 'GRANT USAGE ON SCHEMA public' sent.\n",
            "\n",
            "Security Configuration Summary for role 'recommender_api_user':\n",
            "- The above GRANT statements define the necessary permissions.\n",
            "- If role creation failed due to permissions, these grants would also fail or have no effect until the role is manually created by an admin.\n",
            "- For API connection: role 'recommender_api_user' would need LOGIN privilege and a password, or other authentication method.\n",
            "\n",
            "--- End of Cell 9 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 10: CAPSTONE CONCLUSION & NEXT STEPS\n",
        "#@markdown Summarizes the project, its alignment with course objectives, and potential future work.\n",
        "print(\"=\"*80)\n",
        "print(\"CAPSTONE PROJECT CONCLUSION: End-to-End Product Recommender with PostgresML\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n🎉 Congratulations on completing the Product Recommender Capstone Project! 🎉\")\n",
        "\n",
        "print(\"\\n--- Summary of Achievements ---\")\n",
        "print(\"This project successfully demonstrated an end-to-end workflow for building a recommendation system using PostgresML, aligning with the course content:\")\n",
        "print(\"1.  **Data Foundation:** Generated and structured synthetic data for users, products, and their interactions within PostgreSQL.\")\n",
        "print(\"2.  **Feature Engineering in SQL:** Created numerical feature arrays for users and products, preparing data for machine learning.\")\n",
        "print(\"3.  **In-Database Model Training:** Leveraged `pgml.train` to train a regression model (`product_recommender_v1`) directly on data residing in PostgreSQL, predicting user preferences (ratings).\")\n",
        "print(\"4.  **Real-time Recommendation Logic:** Developed a PL/pgSQL function (`get_recommendations`) that uses the trained model via `pgml.predict` to score products for a given user and return top recommendations.\")\n",
        "print(\"5.  **API Design for Integration:** Designed a FastAPI application to expose the recommendation logic as a consumable web service, including considerations for database connection pooling.\")\n",
        "print(\"6.  **Basic Production Considerations:** Implemented foundational SQL structures for monitoring prediction requests and for database security (role-based access control).\")\n",
        "\n",
        "print(\"\\n--- Alignment with Course Objectives & Capstone Goals ---\")\n",
        "print(\"This project directly applied key concepts from the course:\")\n",
        "print(\"- **Core PostgresML:** Utilized `pgml.train` for training and `pgml.predict` for inference.\")\n",
        "print(\"- **Feature Engineering:** Transformed raw data into `FLOAT[]` features suitable for the ML model.\")\n",
        "print(\"- **Model Deployment & Integration (Design):** While not running a live server in Colab, the FastAPI code and SQL function represent the integration of the ML model into an application layer.\")\n",
        "print(\"- **Production Insights:** The monitoring table/view and security role setup illustrate initial steps towards production readiness.\")\n",
        "print(\"- **Interactive Application (Conceptual):** The `get_recommendations` function serves as the backend for an interactive recommendation application.\")\n",
        "print(\"\\nWhile a full-scale production deployment with live A/B testing, a sophisticated UI dashboard, and fully automated retraining pipelines is beyond the scope of this single notebook exercise, this capstone establishes the fundamental building blocks and demonstrates the power of PostgresML as an MLOps platform within the database.\")\n",
        "\n",
        "print(\"\\n--- Potential Future Enhancements & Real-World Extensions ---\")\n",
        "print(\"- **Advanced Feature Engineering:** Incorporate text embeddings (e.g., from product descriptions using `pgml.embed` if available or `pgml.transform` if a suitable model is enabled) or image features.\")\n",
        "print(\"- **More Sophisticated Models:** Explore other algorithms supported by `pgml.train` (e.g., classification for 'did purchase' vs 'did not', or ranking models if available). Investigate different regression algorithms.\")\n",
        "print(\"- **Implicit Feedback:** Adapt the model to use implicit feedback (views, add-to-carts) if explicit ratings are sparse.\")\n",
        "print(\"- **Cold Start Problem:** Implement strategies for new users or new items (e.g., content-based recommendations as a fallback).\")\n",
        "print(\"- **Full API Deployment:** Deploy the FastAPI application to a cloud service (e.g., AWS Lambda, Google Cloud Run, Heroku).\")\n",
        "print(\"- **Comprehensive Monitoring:** Integrate with tools like Prometheus/Grafana for richer monitoring dashboards.\")\n",
        "print(\"- **Automated Retraining:** Use `pg_cron` or an external orchestrator (Airflow, Kubeflow Pipelines) to schedule `pgml.train` with updated data.\")\n",
        "print(\"- **A/B Testing Framework:** Implement a system to serve different recommendation models to segments of users and compare their performance.\")\n",
        "\n",
        "print(\"\\nThis capstone project provides a solid foundation and understanding of how to build and integrate machine learning models for recommendation tasks directly within a PostgreSQL environment using PostgresML. Well done!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnN9RdltQYZg",
        "outputId": "bb9e1046-aabf-49b2-fec7-e61a21895df9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "CAPSTONE PROJECT CONCLUSION: End-to-End Product Recommender with PostgresML\n",
            "================================================================================\n",
            "\n",
            "🎉 Congratulations on completing the Product Recommender Capstone Project! 🎉\n",
            "\n",
            "--- Summary of Achievements ---\n",
            "This project successfully demonstrated an end-to-end workflow for building a recommendation system using PostgresML, aligning with the course content:\n",
            "1.  **Data Foundation:** Generated and structured synthetic data for users, products, and their interactions within PostgreSQL.\n",
            "2.  **Feature Engineering in SQL:** Created numerical feature arrays for users and products, preparing data for machine learning.\n",
            "3.  **In-Database Model Training:** Leveraged `pgml.train` to train a regression model (`product_recommender_v1`) directly on data residing in PostgreSQL, predicting user preferences (ratings).\n",
            "4.  **Real-time Recommendation Logic:** Developed a PL/pgSQL function (`get_recommendations`) that uses the trained model via `pgml.predict` to score products for a given user and return top recommendations.\n",
            "5.  **API Design for Integration:** Designed a FastAPI application to expose the recommendation logic as a consumable web service, including considerations for database connection pooling.\n",
            "6.  **Basic Production Considerations:** Implemented foundational SQL structures for monitoring prediction requests and for database security (role-based access control).\n",
            "\n",
            "--- Alignment with Course Objectives & Capstone Goals ---\n",
            "This project directly applied key concepts from the course:\n",
            "- **Core PostgresML:** Utilized `pgml.train` for training and `pgml.predict` for inference.\n",
            "- **Feature Engineering:** Transformed raw data into `FLOAT[]` features suitable for the ML model.\n",
            "- **Model Deployment & Integration (Design):** While not running a live server in Colab, the FastAPI code and SQL function represent the integration of the ML model into an application layer.\n",
            "- **Production Insights:** The monitoring table/view and security role setup illustrate initial steps towards production readiness.\n",
            "- **Interactive Application (Conceptual):** The `get_recommendations` function serves as the backend for an interactive recommendation application.\n",
            "\n",
            "While a full-scale production deployment with live A/B testing, a sophisticated UI dashboard, and fully automated retraining pipelines is beyond the scope of this single notebook exercise, this capstone establishes the fundamental building blocks and demonstrates the power of PostgresML as an MLOps platform within the database.\n",
            "\n",
            "--- Potential Future Enhancements & Real-World Extensions ---\n",
            "- **Advanced Feature Engineering:** Incorporate text embeddings (e.g., from product descriptions using `pgml.embed` if available or `pgml.transform` if a suitable model is enabled) or image features.\n",
            "- **More Sophisticated Models:** Explore other algorithms supported by `pgml.train` (e.g., classification for 'did purchase' vs 'did not', or ranking models if available). Investigate different regression algorithms.\n",
            "- **Implicit Feedback:** Adapt the model to use implicit feedback (views, add-to-carts) if explicit ratings are sparse.\n",
            "- **Cold Start Problem:** Implement strategies for new users or new items (e.g., content-based recommendations as a fallback).\n",
            "- **Full API Deployment:** Deploy the FastAPI application to a cloud service (e.g., AWS Lambda, Google Cloud Run, Heroku).\n",
            "- **Comprehensive Monitoring:** Integrate with tools like Prometheus/Grafana for richer monitoring dashboards.\n",
            "- **Automated Retraining:** Use `pg_cron` or an external orchestrator (Airflow, Kubeflow Pipelines) to schedule `pgml.train` with updated data.\n",
            "- **A/B Testing Framework:** Implement a system to serve different recommendation models to segments of users and compare their performance.\n",
            "\n",
            "This capstone project provides a solid foundation and understanding of how to build and integrate machine learning models for recommendation tasks directly within a PostgreSQL environment using PostgresML. Well done!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 11: CLEANUP (OPTIONAL)\n",
        "#@markdown Drops all tables, functions, roles, and schemas created during this capstone project.\n",
        "#@markdown **Check the box and run this cell to perform cleanup.**\n",
        "print(\"--- Optional Database Cleanup ---\")\n",
        "perform_cleanup = False #@param {type:\"boolean\"}\n",
        "\n",
        "if perform_cleanup:\n",
        "    print(\"\\n🧹 Performing cleanup as requested...\")\n",
        "\n",
        "    # Functions\n",
        "    print(\"  Dropping SQL function 'get_recommendations'...\")\n",
        "    run_sql(\"DROP FUNCTION IF EXISTS get_recommendations(INTEGER, INTEGER);\", fetch_results=False, quiet=True)\n",
        "    print(\"  Dropping SQL function 'process_features' (if created from course slides)...\")\n",
        "    run_sql(\"DROP FUNCTION IF EXISTS process_features(JSONB);\", fetch_results=False, quiet=True)\n",
        "\n",
        "    # Monitoring objects (view first, then table, then schema)\n",
        "    print(\"  Dropping monitoring objects...\")\n",
        "    run_sql(\"DROP VIEW IF EXISTS monitoring.metrics_view CASCADE;\", fetch_results=False, quiet=True)\n",
        "    run_sql(\"DROP TABLE IF EXISTS monitoring.predictions CASCADE;\", fetch_results=False, quiet=True)\n",
        "    run_sql(\"DROP SCHEMA IF EXISTS monitoring CASCADE;\", fetch_results=False, quiet=True)\n",
        "\n",
        "    # Core data and feature tables (order by dependency or use CASCADE)\n",
        "    print(\"  Dropping data and feature tables...\")\n",
        "    run_sql(\"DROP TABLE IF EXISTS training_data CASCADE;\", fetch_results=False, quiet=True)\n",
        "    run_sql(\"DROP TABLE IF EXISTS interactions CASCADE;\", fetch_results=False, quiet=True)\n",
        "    run_sql(\"DROP TABLE IF EXISTS user_features CASCADE;\", fetch_results=False, quiet=True)\n",
        "    run_sql(\"DROP TABLE IF EXISTS product_features CASCADE;\", fetch_results=False, quiet=True)\n",
        "    run_sql(\"DROP TABLE IF EXISTS users CASCADE;\", fetch_results=False, quiet=True)\n",
        "    run_sql(\"DROP TABLE IF EXISTS products CASCADE;\", fetch_results=False, quiet=True)\n",
        "\n",
        "    # Roles (ensure role doesn't own objects or have dependencies not covered by CASCADE on tables/functions)\n",
        "    API_ROLE_NAME_CLEANUP = \"recommender_api_user\" # Ensure this matches the role name used\n",
        "    print(f\"  Dropping role '{API_ROLE_NAME_CLEANUP}'...\")\n",
        "    try:\n",
        "        run_sql(f\"DROP ROLE IF EXISTS {API_ROLE_NAME_CLEANUP};\", fetch_results=False, quiet=True)\n",
        "    except Exception as e_role_final_drop:\n",
        "        print(f\"  ⚠️ Note: Could not drop role '{API_ROLE_NAME_CLEANUP}' directly. It might require manual intervention if it still has privileges or owns objects. Error: {e_role_final_drop}\")\n",
        "\n",
        "\n",
        "    # PGML Project (Optional - pgml.train creates a project)\n",
        "    # `pgml.delete_project` or `pgml.archive_project` might be needed if you want to fully remove it.\n",
        "    # This is less critical for a demo unless project names clash.\n",
        "    # Example: run_sql(f\"SELECT pgml.archive_project('{PROJECT_NAME}');\", fetch_results=False, quiet=True)\n",
        "\n",
        "    print(\"\\n✅ Cleanup operations attempted. Please verify in your database client if needed.\")\n",
        "else:\n",
        "    print(\"\\nCleanup skipped as 'perform_cleanup' checkbox is not checked.\")\n",
        "\n",
        "print(\"\\n--- End of Cleanup Cell ---\")"
      ],
      "metadata": {
        "id": "jvOv6FtVQaOU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}